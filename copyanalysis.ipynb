{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "eec5a4ca",
      "cell_type": "code",
      "source": "#PCA METHOD WITH REPORTING\nimport pandas as pd\nimport numpy as np\nimport math\nimport networkx as nx\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\ndef reduce_pca_by_variance(data: np.ndarray, feature_names: list, variance_threshold: float):\n    \"\"\"\n    Performs PCA on n-dimensional data, automatically selecting the minimum\n    number of components to explain at least the `variance_threshold`.\n    \n    This modified version also prints the results of the reduction and\n    the top 5 feature contributors for each component.\n\n    Args:\n        data: A (n_samples, n_features) NumPy array.\n        feature_names: A list of strings corresponding to the feature columns\n                       in `data`. (e.g., list(df.columns))\n        variance_threshold: The target amount of variance to explain\n                            (e.g., 0.95 for 95%).\n\n    Returns:\n        A tuple containing:\n        - data_transformed (np.ndarray): The data projected onto the\n                                         new component space.\n        - fitted_pca (PCA): The fitted PCA object, which you can use\n                            to inspect the number of components, etc.\n        - explained_variance_list (list): A list of the variance explained\n                                          by each component (e.g., [0.5, 0.3]).\n    \"\"\"\n    \n    if len(feature_names) != data.shape[1]:\n        raise ValueError(f\"Number of feature_names ({len(feature_names)}) does not \"\n                         f\"match number of data columns ({data.shape[1]}).\")\n\n    # 1. Create a PCA object with the variance threshold.\n    # By setting n_components to a float, PCA automatically finds\n    # the components needed to explain that much variance.\n    pca = PCA(n_components=variance_threshold)\n    \n    # 2. Create a pipeline to first scale the data, then run PCA.\n    # Scaling is crucial for PCA to work correctly.\n    pipeline = Pipeline([\n        ('scaler', StandardScaler()),\n        ('pca', pca)\n    ])\n    \n    # 3. Fit the pipeline to the data and transform it\n    data_transformed = pipeline.fit_transform(data)\n    \n    # --- Report print statements ---\n    \n    # Get the original and new dimensions\n    original_dimensions = data.shape[1]\n    # We access the fitted pca object from step 2\n    new_dimensions = pca.n_components_ \n    \n    print(\"-\" * 30)\n    print(\"PCA Dimensionality Reduction Report\")\n    print(\"-\" * 30)\n    print(f\"Original dimensions:   {original_dimensions}\")\n    print(f\"New dimensions:        {new_dimensions}\")\n    print(f\"Dimensions reduced by: {original_dimensions - new_dimensions}\")\n    print(\"\\nVariance explained by each remaining component:\")\n    \n    # pca.explained_variance_ratio_ is an array like [0.5, 0.3, 0.1]\n    for i, variance in enumerate(pca.explained_variance_ratio_):\n        print(f\"  Principal Component {i+1}: {variance * 100:.2f}%\")\n        \n    # Print total variance explained\n    total_variance = np.sum(pca.explained_variance_ratio_)\n    print(f\"\\nTotal variance explained: {total_variance * 100:.2f}%\")\n    print(f\"(Target threshold was {variance_threshold * 100:.0f}%)\")\n    \n    # --- New: Top 5 Contributors per Component Report ---\n    print(\"\\nTop 5 Contributors per Component:\")\n    \n    # pca.components_ has shape (n_components, n_features)\n    for i, component in enumerate(pca.components_):\n        print(f\"  --- Principal Component {i+1} ---\")\n        \n        # Get indices of the top 5 absolute loadings\n        # np.argsort returns indices of smallest to largest\n        # We take the last 5, and then reverse them [::-1]\n        top_5_indices = np.argsort(np.abs(component))[-5:][::-1]\n        \n        # Print the feature name and its loading (weight)\n        for j, feature_index in enumerate(top_5_indices):\n            feature_name = feature_names[feature_index]\n            loading = component[feature_index]\n            print(f\"    {j+1}. {feature_name}: {loading:.4f}\")\n            \n    print(\"-\" * 30)\n    \n    # --- End of report ---\n    \n    # Get the list of explained variances\n    explained_variance_list = pca.explained_variance_ratio_.tolist()\n    \n    # Return the new data, the fitted PCA object, and the list of variances\n    return data_transformed, pca, explained_variance_list\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "id": "dddeee11",
      "cell_type": "markdown",
      "source": "# Processing",
      "metadata": {}
    },
    {
      "id": "a61a8515",
      "cell_type": "code",
      "source": "# Get data to be a numpy array probably\ndf=pd.read_csv('consolidated_acs_data_clean.csv')\ndata=np.array(df)\nincome_index=1  # Example index for income dimension\n# Transform the income dimension by logarithmic scale\ndata[:, income_index] = np.log(data[:, income_index].astype(np.float64))\n# Normalize data by dividing by standard deviation by dimension\nfor i in range(1,data.shape[1]):\n    data[:, i] = data[:, i] / np.std(data[:, i])\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "id": "6b78064e",
      "cell_type": "markdown",
      "source": "## Weighting",
      "metadata": {}
    },
    {
      "id": "40bdd910",
      "cell_type": "code",
      "source": "# Initilize weights, and scale data by weights\n# Get column indices\nincome_index = 1  # per_capita_income\npoverty_index = df.columns.get_loc('avg_poverty_ratio')\nemployment_index = df.columns.get_loc('in_labor_force')\nhealthcare_index = df.columns.get_loc('Educational services, and health care and social assistance')\neducation_index = df.columns.get_loc('College or More')\ncommute_index = df.columns.get_loc('avg_commute_time')\nhousing_index = df.columns.get_loc('avg_housing_cost_burden')\nrace_indices = [df.columns.get_loc(col) for col in ['white', 'black', 'asian', 'native', 'pacific islander', 'other']]\n\n# Apply dimension weights\ndata[:, income_index] *= 0.1225          # Economic Security - Income\ndata[:, poverty_index] *= 0.0525         # Economic Security - Poverty\ndata[:, employment_index] *= 0.07        # Economic Security - Employment\ndata[:, healthcare_index] *= 0.105       # Economic Security - Healthcare\ndata[:, education_index] *= 0.15         # Education\ndata[:, commute_index] *= 0.15           # Location Affordability - Transportation\ndata[:, housing_index] *= 0.15           # Location Affordability - Housing\n\n# Race: distribute 0.2 across 6 race columns\nfor idx in race_indices:\n    data[:, idx] *= 0.2 / 6  # 0.0333 per column\n\nweights = np.array([\n    0.1225,     # 1:  per_capita_income (Economic Security - Income)\n    0.2/6,      # 2:  white (Cultural - Race)\n    0.2/6,      # 3:  black (Cultural - Race)\n    0.2/6,      # 4:  asian (Cultural - Race)\n    0.2/6,      # 5:  native (Cultural - Race)\n    0.2/6,      # 6:  pacific islander (Cultural - Race)\n    0.2/6,      # 7:  other (Cultural - Race)\n    0.0,        # 8:  Under High School (not weighted)\n    0.0,        # 9:  High School (No College Degree) (not weighted)\n    0.15,       # 10: College or More (Education)\n    0.0,        # 11: Agriculture (not weighted)\n    0.0,        # 12: Construction_and_manufacturing (not weighted)\n    0.0,        # 13: trade (not weighted)\n    0.0,        # 14: Transportation and warehousing (not weighted)\n    0.0,        # 15: nerds (not weighted)\n    0.105,      # 16: Educational services, and health care (Economic Security - Healthcare)\n    0.0,        # 17: finance_inurance_and_realty (not weighted)\n    0.0,        # 18: other_services (not weighted)\n    0.07,       # 19: in_labor_force (Economic Security - Employment)\n    0.0,        # 20: out_labor_force (not weighted)\n    0.15,       # 21: avg_commute_time (Location Affordability - Transportation)\n    0.15,       # 22: avg_housing_cost_burden (Location Affordability - Housing)\n    0.0525,     # 23: avg_poverty_ratio (Economic Security - Poverty)\n])\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "id": "2d4f928a",
      "cell_type": "code",
      "source": "# Perform PCA, and project onto the top N dimensions so that they explain 50% of the variance\nnew_data,pca,var_explained = reduce_pca_by_variance(data[:,1:], list(df.columns)[1:], variance_threshold=0.5)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "------------------------------\nPCA Dimensionality Reduction Report\n------------------------------\nOriginal dimensions:   23\nNew dimensions:        6\nDimensions reduced by: 17\n\nVariance explained by each remaining component:\n  Principal Component 1: 16.07%\n  Principal Component 2: 10.11%\n  Principal Component 3: 8.43%\n  Principal Component 4: 6.79%\n  Principal Component 5: 6.32%\n  Principal Component 6: 5.67%\n\nTotal variance explained: 53.38%\n(Target threshold was 50%)\n\nTop 5 Contributors per Component:\n  --- Principal Component 1 ---\n    1. College or More: 0.4448\n    2. avg_poverty_ratio: 0.3872\n    3. Under High School: -0.3443\n    4. per_capita_income: 0.2592\n    5. in_labor_force: 0.2370\n  --- Principal Component 2 ---\n    1. out_labor_force: -0.4101\n    2. in_labor_force: 0.4101\n    3. white: -0.3803\n    4. pacific islander: -0.3752\n    5. Agriculture, forestry, fishing and hunting, and mining: -0.2772\n  --- Principal Component 3 ---\n    1. High School (No College Degree): 0.4196\n    2. avg_housing_cost_burden: -0.4130\n    3. white: 0.3610\n    4. Construction_and_manufacturing: 0.3370\n    5. asian: -0.2544\n  --- Principal Component 4 ---\n    1. other: 0.4171\n    2. Under High School: 0.3589\n    3. black: -0.3423\n    4. Educational services, and health care and social assistance: -0.2938\n    5. out_labor_force: -0.2873\n  --- Principal Component 5 ---\n    1. other: 0.4635\n    2. in_labor_force: -0.4026\n    3. out_labor_force: 0.4026\n    4. pacific islander: -0.4001\n    5. native: -0.2099\n  --- Principal Component 6 ---\n    1. other_services: 0.6451\n    2. black: -0.3944\n    3. avg_commute_time: -0.3466\n    4. Transportation and warehousing, and utilities: -0.3019\n    5. white: 0.2319\n------------------------------\n"
        }
      ],
      "execution_count": 5
    },
    {
      "id": "deb9dd36",
      "cell_type": "markdown",
      "source": "As shown, the first component has a distinct identity, namely education, income and poverty, part of the most important factors typically considered as determinant of a person's sociol-economic status. The rest of the components are a mixed of race, education, occupation, and comute time.",
      "metadata": {}
    },
    {
      "id": "e351f034",
      "cell_type": "markdown",
      "source": "# Network initialization",
      "metadata": {}
    },
    {
      "id": "84b89226",
      "cell_type": "code",
      "source": "def create_dimension_layered_knn(data, dimension_weights, k=10):\n    \"\"\"\n    Create multi-layer network where each dimension has its own KNN graph.\n    \"\"\"\n    G = nx.Graph()\n    \n    # Add nodes\n    for i in range(data.shape[0]):\n        G.add_node(i)\n    \n    # For each dimension, create KNN graph\n    for dim in range(data.shape[1]):\n        dim_weight = dimension_weights[dim]\n        \n        if dim_weight == 0:\n            continue\n        \n        # Get this dimension's values (1D)\n        dim_data = data[:, dim].reshape(-1, 1)\n        \n        # Build KNN graph for THIS dimension only\n        from sklearn.neighbors import NearestNeighbors\n        nbrs = NearestNeighbors(n_neighbors=k)\n        nbrs.fit(dim_data)\n        distances, indices = nbrs.kneighbors(dim_data)\n        \n        # Add edges weighted by dimension importance\n        for i in range(len(data)):\n            for j, neighbor in enumerate(indices[i]):\n                if i != neighbor:\n                    if G.has_edge(i, neighbor):\n                        G[i][neighbor]['weight'] += dim_weight  # Accumulate\n                    else:\n                        G.add_edge(i, neighbor, weight=dim_weight)\n    \n    return G\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "id": "eba930f8",
      "cell_type": "code",
      "source": "# from ring_network import construct_net\nG = create_dimension_layered_knn(new_data, dimension_weights=var_explained, k=10)\nG_uni= nx.Graph()\nG_uni.add_nodes_from(G.nodes(data=True))\nG_uni.add_edges_from(G.edges())\n\n# Gring= construct_net(new_data,var_explained,0.2)\n# Gring_uni= nx.Graph()\n# Gring_uni.add_nodes_from(Gring.nodes(data=True))\n# Gring_uni.add_edges_from(Gring.edges())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "id": "7250c6c9",
      "cell_type": "markdown",
      "source": "## Geographic weighting",
      "metadata": {}
    },
    {
      "id": "a1161760",
      "cell_type": "code",
      "source": "import pickle\nfrom scipy.sparse import load_npz\n\n# Create graph\nGeo = G.copy()\n#Gring_geo=Gring.copy()\n# Load adjacency matrix\nadj_matrix = load_npz('adjacency_queen_matrix.npz')\n\n# Load mappings\nwith open('adjacency_queen_mappings.pkl', 'rb') as f:\n    mappings = pickle.load(f)\nindex_to_geoid = mappings['index_to_geoid']\n\n# Get edges from sparse matrix\nrows, cols = adj_matrix.nonzero()\n\n# Add edges with your weight\nweight = 2  # Change this to your desired weight\n\nfor i, j in zip(rows, cols):\n    if i < j:  # Only add each edge once (undirected)\n        geoid1 = index_to_geoid[i]\n        geoid2 = index_to_geoid[j]\n        Geo.add_edge(i, j, weight=weight)\n        #Gring_geo.add_edge(i, j, weight=weight)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "id": "2768f93b",
      "cell_type": "markdown",
      "source": "# Grouping",
      "metadata": {}
    },
    {
      "id": "4cc3c755",
      "cell_type": "code",
      "source": "# K means clustering function\nfrom sklearn.cluster import KMeans\n\ndef get_kmeans_partition(data: np.ndarray, weights, n_clusters=14):\n    \"\"\"\n    Runs K-means clustering on the input data and returns the loss\n    (inertia) and a partition of the data indices by cluster.\n\n    Args:\n        data: A (n_samples, n_features) NumPy array.\n        n_clusters: The number of clusters (k).\n\n    Returns:\n        A tuple containing:\n        - loss (float): The inertia (Within-Cluster Sum of Squares).\n        - partitions (dict): A dictionary where keys are cluster IDs (0 to k-1)\n                             and values are lists of original data indices\n                             belonging to that cluster.\n    \"\"\"\n    \n    # 1. Initialize and fit the K-means model\n    # n_init=10 runs the algorithm 10 times and picks the best result\n    # random_state=42 ensures the result is reproducible\n    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n    for i in range(len(weights)):\n        data[:, i] = data[:, i] * weights[i]\n    kmeans.fit(data)\n\n    # 2. Get the loss (inertia)\n    # .inertia_ is the WCSS (Within-Cluster Sum of Squares)\n    loss = kmeans.inertia_\n\n    # 3. Get the cluster assignment for each data point\n    # .labels_ is an array like [0, 1, 1, 0, 2, ...]\n    labels = kmeans.labels_\n\n    # 4. Create the partition of indices\n    partitions = {i: [] for i in range(n_clusters)}\n    for index, cluster_id in enumerate(labels):\n        partitions[cluster_id].append(index)\n\n    return loss, partitions\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\ndef calculate_kmeans_loss_for_partition(data: np.ndarray, partition: list) -> float:\n    \"\"\"\n    Calculates the K-means \"loss\" (Inertia, or Within-Cluster Sum of Squares)\n    for a given dataset and a user-provided partition.\n\n    Args:\n        data: A (n_samples, n_features) NumPy array.\n        partition: A list of lists, where each inner list contains the\n                   *indices* (row numbers) of the data points belonging\n                   to that cluster.\n                   Example: [[0, 1, 4], [2, 3, 5]]\n\n    Returns:\n        total_loss (float): The total K-means loss (Inertia) for this partition.\n    \"\"\"\n    \n    total_loss = 0.0\n\n    # Iterate over each cluster (which is a list of indices)\n    for indices in partition:\n        \n        # 1. Get all data points belonging to this cluster\n        # Using [indices, :] selects all rows whose index is in the list\n        cluster_points = data[indices, :]\n        \n        # Handle empty clusters (their loss is 0)\n        if cluster_points.shape[0] == 0:\n            continue\n            \n        # 2. Calculate the \"true\" centroid (mean) for this cluster\n        # axis=0 calculates the mean of each *column* (feature)\n        centroid = np.mean(cluster_points, axis=0)\n        \n        # 3. Calculate the sum of squared distances from each point to the centroid\n        #    - (cluster_points - centroid) uses broadcasting to get distance vectors\n        #    - (** 2) squares all distances\n        #    - np.sum(...) sums all squared distances into a single number\n        cluster_loss = np.sum((cluster_points - centroid) ** 2)\n        \n        # 4. Add this cluster's loss to the total\n        total_loss += cluster_loss\n        \n    return total_loss\n\n\n\n\nk = 14\n\n# 3. Run the function\ntotal_loss, index_partitions = get_kmeans_partition(new_data,weights=var_explained, n_clusters=k)\n\nindex_partitions=[index_partitions[i] for i in range(k)]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "id": "463ea6ed",
      "cell_type": "code",
      "source": "partG=nx.algorithms.community.louvain_communities(G,weight='weight',resolution=0.75)\nprint(\"Number of Louvain communities G:\",len(partG))\npartGeo=nx.algorithms.community.louvain_communities(Geo,weight='weight',resolution=0.8)\nprint(\"Number of Louvain communities Geo:\",len(partGeo))\npartG_uni=nx.algorithms.community.louvain_communities(G_uni,weight='weight',resolution=0.95)\nprint(\"Number of Louvain communities G_uni:\",len(partG_uni))\n\n# partGring=nx.algorithms.community.louvain_communities(Gring,weight='weight',resolution=0.65)\n# print(\"Number of Louvain communities ring:\",len(partGring))\n# partRing_geo=nx.algorithms.community.louvain_communities(Gring_geo,weight='weight',resolution=0.7)\n# print(\"Number of Louvain communities ring geo:\",len(partRing_geo))\n# partRing_uni=nx.algorithms.community.louvain_communities(Gring_uni,resolution=0.75)\n# print(\"Number of Louvain communities ring uni:\",len(partRing_uni))",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Number of Louvain communities G: 18\nNumber of Louvain communities Geo: 22\nNumber of Louvain communities G_uni: 18\n"
        }
      ],
      "execution_count": 10
    },
    {
      "id": "3d6c94e4",
      "cell_type": "code",
      "source": "print(np.std([len(x) for x in partGeo]))",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "126.07715235960653\n"
        }
      ],
      "execution_count": 11
    },
    {
      "id": "600c8e70",
      "cell_type": "code",
      "source": "print(\"\\n 10Graph Comparisons\")\nprint(\"Louvain modularity G:\",nx.algorithms.community.modularity(G, partG, weight='weight'))\nprint(\"Louvain modularity Geo partition on G:\",nx.algorithms.community.modularity(Geo, partGeo, weight='weight'))\nprint(\"Louvain modularity Uni partition on G:\",nx.algorithms.community.modularity(G, partG_uni, weight='weight'))\n\n# print(\"\\n Ring Comparisons\")\n# print(\"Louvain modularity ring:\",nx.algorithms.community.modularity(Gring, partGring, weight='weight'))\n# print(\"Louvain modularity Geo partition on ring:\",nx.algorithms.community.modularity(Gring, partRing_geo, weight='weight'))\n# print(\"Louvain modularity Uni partition on Gring:\",nx.algorithms.community.modularity(Gring, partRing_uni, weight='weight'))\n\nprint(\"\\n K Means Comparisons\")\nprint(\"K means modularity on G:\",nx.algorithms.community.modularity(G, index_partitions, weight='weight'))\n# print(\"K means modularity on ring:\",nx.algorithms.community.modularity(Gring, index_partitions, weight='weight'))",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\n 10Graph Comparisons\nLouvain modularity G: 0.2880053616235307\nLouvain modularity Geo partition on G: 0.5171767219612027\nLouvain modularity Uni partition on G: 0.15997793457030196\n\n K Means Comparisons\nK means modularity on G: 0.10560605648755736\n"
        }
      ],
      "execution_count": 12
    },
    {
      "id": "57f9fb8d",
      "cell_type": "code",
      "source": "\n# 1. Load your real data (as you were before)\n\n# 2. Get your partition\n# (e.g., from K-means, Louvain, or any other method)\n# This is just a placeholder example!\n# You would have your *real* partition here.\n\n# 3. Set the path to the shapefile you downloaded\n# (Update this path to where you unzipped the file)\nSHAPEFILE = \"tl_2020_37_tract/tl_2020_37_tract.shp\"\n\n# 4. Call the plotting function\n# from plot_geo import *\n# plot_partition_geographically(data, partGeo, SHAPEFILE)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "id": "e872f88b",
      "cell_type": "code",
      "source": "# color=[0]*data.shape[0]\n# for i in range(len(partition)):\n#     for o in partition[i]:\n#         color[o]=i\n# nx.draw(G,node_color=color,node_size=1)\n# pos = nx.spring_layout(G, seed=42)\n# nx.draw_networkx_nodes(G, pos, node_color=color, node_size=1)\n# nx.draw_networkx_edges(G, pos, edge_color='black', alpha=0.1)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "id": "13cf2321-ddc4-4aa4-8f04-076eebe254b4",
      "cell_type": "code",
      "source": "# need initial partition to have 14 groups",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "id": "4d59835f-38f6-4dc7-abdb-1d9f77968e02",
      "cell_type": "code",
      "source": "from typing import List, Set, Dict, Tuple\nimport time\n\n# --- Helper Functions ---\n\ndef partition_to_map(partition: List[Set[int]]) -> Dict[int, int]:\n    \"\"\"Converts a list of sets to a dictionary mapping node -> community_index.\"\"\"\n    node_to_community = {}\n    for i, community_set in enumerate(partition):\n        for node in community_set:\n            node_to_community[node] = i\n    return node_to_community\n\ndef check_contiguity(Geo: nx.Graph, old_community_idx: int, prospective_partition: List[Set[int]]) -> bool:\n    \"\"\"\n    Checks if the community at old_community_idx remains contiguous after a move.\n    \n    Args:\n        Geo: The geographical adjacency graph.\n        old_community_idx: The index of the community to check.\n        prospective_partition: The partition *after* the move has been simulated.\n        \n    Returns:\n        True if the community remains contiguous, False otherwise.\n    \"\"\"\n    \n    remaining_nodes = prospective_partition[old_community_idx]\n    \n    if len(remaining_nodes) <= 1:\n        return True\n        \n    # Get the first node to start the check\n    start_node = next(iter(remaining_nodes))\n    \n    # Perform a graph traversal (BFS) on the subgraph induced by remaining_nodes\n    # use the Geo graph's edges for contiguity.\n    \n    # Create a set of reachable nodes\n    reachable = set()\n    queue = [start_node]\n    \n    while queue:\n        current = queue.pop(0)\n        if current not in reachable:\n            reachable.add(current)\n            # Check neighbors in the Geo graph\n            for neighbor in Geo.neighbors(current):\n                # Only follow edges where the neighbor is still in the community\n                if neighbor in remaining_nodes and neighbor not in reachable:\n                    queue.append(neighbor)\n                    \n    # Check if all remaining nodes were reached\n    return len(reachable) == len(remaining_nodes)\n\n# --- Core Optimization Function with Sequential Swap and Delta Q ---\ndef optimized_sequential_swap(G: nx.Graph, Geo: nx.Graph, initial_partition: List[Set[int]], N_iterations: int) -> Tuple[List[Set[int]], float]:\n    \"\"\"\n    Performs local search optimization using a sequential, targeted two-node swap.\n    In each iteration, it finds the single best swap across all districts and executes it.\n    \"\"\"\n    \n    # 1. Pre-calculate initial community properties\n    best_part = [set(comm) for comm in initial_partition] # current best partition\n    node_to_community_map = partition_to_map(best_part) # convert to map for better searching\n    \n    # Total edge weight of the graph G (2m)\n    total_weight_2m = G.size(weight='weight') * 2\n    \n    # Calculate initial community properties for Delta Q calculation (change in modularity)\n    community_properties = {}\n    for i, community in enumerate(best_part):\n        subgraph = G.subgraph(community)\n        total_internal_weight = subgraph.size(weight='weight')\n        total_degree_sum = sum(G.degree(node, weight='weight') for node in community)\n        \n        community_properties[i] = {\n            'internal_weight': total_internal_weight,\n            'degree_sum': total_degree_sum\n        }\n    \n    # Calculate initial modularity score\n    modularity_part = tuple(frozenset(s) for s in best_part if s)\n    best_score = nx.algorithms.community.modularity(G, modularity_part, weight='weight')\n\n    total_duration = 0.0\n    \n    print(f\"Starting optimization with Modularity Score: {best_score:.4f}\")\n    \n    # --- Main Optimization Loop ---\n    for i in range(N_iterations):\n        start_time = time.time()\n        print(f\"--- Iteration {i+1}/{N_iterations} ---\")\n        \n        best_gain = 0.0\n        best_swap = None  # Stores: (node_A, node_B, comm_A_idx, comm_B_idx)\n        \n        # 2. Iterate through each district (community)\n        # We use a list of indices to ensure we iterate over all districts\n        district_indices = list(range(len(best_part)))\n        \n        # 3. Find the single best swap across ALL districts\n        for comm_A_idx in district_indices:\n            \n            # Get all nodes in the current district\n            district_A_nodes = best_part[comm_A_idx]\n            \n            # Identify all potential swap candidates involving a node from comm_A_idx\n            # A swap candidate is a pair (A, B) where A is in comm_A_idx and B is adjacent to A\n            # and B is in a different community comm_B_idx.\n            \n            # Iterate over all nodes A in the current district\n            for node_A in district_A_nodes:\n                \n                # Iterate over all geographical neighbors of A\n                for node_B in Geo.neighbors(node_A):\n                    \n                    comm_B_idx = node_to_community_map.get(node_B)\n                    \n                    # Check if B is in a different community\n                    if comm_B_idx is not None and comm_B_idx != comm_A_idx:\n                        \n                        # Delta Q change in modularity calculation\n                        \n                        # 1. Calculate gain for moving A from comm_A to comm_B\n                        k_A = G.degree(node_A, weight='weight')\n                        k_A_in_B = sum(G[node_A][neighbor]['weight'] \n                                       for neighbor in G.neighbors(node_A) \n                                       if node_to_community_map.get(neighbor) == comm_B_idx)\n                        \n                        sigma_tot_A = community_properties[comm_A_idx]['degree_sum']\n                        sigma_tot_B = community_properties[comm_B_idx]['degree_sum']\n                        \n                        # Delta Q for A moving to B (ignoring B)\n                        gain_A = (k_A_in_B / total_weight_2m) - (k_A * sigma_tot_B / (total_weight_2m ** 2))\n                        \n                        # 2. Calculate gain for moving B from comm_B to comm_A\n                        k_B = G.degree(node_B, weight='weight')\n                        k_B_in_A = sum(G[node_B][neighbor]['weight'] \n                                       for neighbor in G.neighbors(node_B) \n                                       if node_to_community_map.get(neighbor) == comm_A_idx)\n                        \n                        # Delta Q for B moving to A (ignoring A)\n                        gain_B = (k_B_in_A / total_weight_2m) - (k_B * sigma_tot_A / (total_weight_2m ** 2))\n                        \n                        # 3. Total Swap Gain (Sum of individual gains + correction)\n                        w_AB = G.get_edge_data(node_A, node_B, default={'weight': 0.0})['weight']\n                        total_gain = gain_A + gain_B + (w_AB / total_weight_2m)\n                        \n                        # --- B. Contiguity Check  ---\n                        if total_gain > best_gain:\n                            # Simulate the swap for the contiguity check\n                            prospective_part = [set(comm) for comm in best_part]\n                            \n                            # A leaves comm_A, B leaves comm_B\n                            prospective_part[comm_A_idx].remove(node_A)\n                            prospective_part[comm_B_idx].remove(node_B)\n                            \n                            # A enters comm_B, B enters comm_A\n                            prospective_part[comm_B_idx].add(node_A)\n                            prospective_part[comm_A_idx].add(node_B)\n                            \n                            # Check if BOTH communities remain contiguous\n                            contiguity_A = check_contiguity(Geo, comm_A_idx, prospective_part)\n                            contiguity_B = check_contiguity(Geo, comm_B_idx, prospective_part)\n                            \n                            if contiguity_A and contiguity_B:\n                                # 4. If the gain is the best so far, record the swap\n                                best_gain = total_gain\n                                best_swap = (node_A, node_B, comm_A_idx, comm_B_idx)\n\n        # 5. Apply the single best swap for this iteration\n        if best_gain > 0:\n            node_A, node_B, comm_A_idx, comm_B_idx = best_swap\n            \n            # Update partition sets\n            best_part[comm_A_idx].remove(node_A)\n            best_part[comm_B_idx].remove(node_B)\n            best_part[comm_B_idx].add(node_A)\n            best_part[comm_A_idx].add(node_B)\n            \n            # Update node-to-community map\n            node_to_community_map[node_A] = comm_B_idx\n            node_to_community_map[node_B] = comm_A_idx\n            \n            # Re-calculate properties for the two affected communities\n            for idx in [comm_A_idx, comm_B_idx]:\n                community = best_part[idx]\n                subgraph = G.subgraph(community)\n                community_properties[idx]['internal_weight'] = subgraph.size(weight='weight')\n                community_properties[idx]['degree_sum'] = sum(G.degree(node, weight='weight') for node in community)\n            \n            # Update total modularity score\n            best_score += best_gain\n            \n            print(f\"  Swap accepted! Node {node_A} moved from {comm_A_idx} to {comm_B_idx}, and Node {node_B} moved from {comm_B_idx} to {comm_A_idx}.\")\n            print(f\"  Modularity increased by: {best_gain:.6f}. New score: {best_score:.4f}\")\n            \n        else:\n            print(\"\\nNo local swap increased modularity while maintaining contiguity. Optimization finished.\")\n            break\n            \n        end_time = time.time()\n        duration = end_time - start_time\n        print(f\"  Iteration {i+1} took: {duration:.2f} seconds\")\n        total_duration += duration\n            \n    print(\"\\n--- Final Results ---\")\n    print(f\"Optimal Modularity Score (after local swaps): {best_score:.4f}\")\n    return best_part, best_score, total_duration\n\n# --- Example Usage (Requires G, Geo, and partGeo to be defined in the notebook) ---\n# best_partition, final_score = optimized_local_search(G, Geo, partGeo, N_iterations=100)\n# print(f\"Final Partition Size: {len(best_partition)}\")\n# print(f\"Final Modularity Score: {final_score}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "id": "5841fb6b-18d5-4127-aba1-962727386c81",
      "cell_type": "code",
      "source": "best_partition, final_score, duration = optimized_local_search(G, Geo, partGeo, N_iterations=10)\nprint(f\"Final Partition Size: {len(best_partition)}\")\nprint(f\"Final Modularity Score: {final_score}\")\nprint(f\"Optimization Time: {duration}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b4c73a73-e2e2-4b0b-8661-05d9c2976fcf",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}