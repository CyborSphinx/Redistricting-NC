{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "eec5a4ca",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#PCA METHOD WITH REPORTING\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import networkx as nx\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "def reduce_pca_by_variance(data: np.ndarray, feature_names: list, variance_threshold: float):\n",
        "    \"\"\"\n",
        "    Performs PCA on n-dimensional data, automatically selecting the minimum\n",
        "    number of components to explain at least the `variance_threshold`.\n",
        "    \n",
        "    This modified version also prints the results of the reduction and\n",
        "    the top 5 feature contributors for each component.\n",
        "\n",
        "    Args:\n",
        "        data: A (n_samples, n_features) NumPy array.\n",
        "        feature_names: A list of strings corresponding to the feature columns\n",
        "                       in `data`. (e.g., list(df.columns))\n",
        "        variance_threshold: The target amount of variance to explain\n",
        "                            (e.g., 0.95 for 95%).\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - data_transformed (np.ndarray): The data projected onto the\n",
        "                                         new component space.\n",
        "        - fitted_pca (PCA): The fitted PCA object, which you can use\n",
        "                            to inspect the number of components, etc.\n",
        "        - explained_variance_list (list): A list of the variance explained\n",
        "                                          by each component (e.g., [0.5, 0.3]).\n",
        "    \"\"\"\n",
        "    \n",
        "    if len(feature_names) != data.shape[1]:\n",
        "        raise ValueError(f\"Number of feature_names ({len(feature_names)}) does not \"\n",
        "                         f\"match number of data columns ({data.shape[1]}).\")\n",
        "\n",
        "    # 1. Create a PCA object with the variance threshold.\n",
        "    # By setting n_components to a float, PCA automatically finds\n",
        "    # the components needed to explain that much variance.\n",
        "    pca = PCA(n_components=variance_threshold)\n",
        "    \n",
        "    # 2. Create a pipeline to first scale the data, then run PCA.\n",
        "    # Scaling is crucial for PCA to work correctly.\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('pca', pca)\n",
        "    ])\n",
        "    \n",
        "    # 3. Fit the pipeline to the data and transform it\n",
        "    data_transformed = pipeline.fit_transform(data)\n",
        "    \n",
        "    # --- Report print statements ---\n",
        "    \n",
        "    # Get the original and new dimensions\n",
        "    original_dimensions = data.shape[1]\n",
        "    # We access the fitted pca object from step 2\n",
        "    new_dimensions = pca.n_components_ \n",
        "    \n",
        "    print(\"-\" * 30)\n",
        "    print(\"PCA Dimensionality Reduction Report\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Original dimensions:   {original_dimensions}\")\n",
        "    print(f\"New dimensions:        {new_dimensions}\")\n",
        "    print(f\"Dimensions reduced by: {original_dimensions - new_dimensions}\")\n",
        "    print(\"\\nVariance explained by each remaining component:\")\n",
        "    \n",
        "    # pca.explained_variance_ratio_ is an array like [0.5, 0.3, 0.1]\n",
        "    for i, variance in enumerate(pca.explained_variance_ratio_):\n",
        "        print(f\"  Principal Component {i+1}: {variance * 100:.2f}%\")\n",
        "        \n",
        "    # Print total variance explained\n",
        "    total_variance = np.sum(pca.explained_variance_ratio_)\n",
        "    print(f\"\\nTotal variance explained: {total_variance * 100:.2f}%\")\n",
        "    print(f\"(Target threshold was {variance_threshold * 100:.0f}%)\")\n",
        "    \n",
        "    # --- New: Top 5 Contributors per Component Report ---\n",
        "    print(\"\\nTop 5 Contributors per Component:\")\n",
        "    \n",
        "    # pca.components_ has shape (n_components, n_features)\n",
        "    for i, component in enumerate(pca.components_):\n",
        "        print(f\"  --- Principal Component {i+1} ---\")\n",
        "        \n",
        "        # Get indices of the top 5 absolute loadings\n",
        "        # np.argsort returns indices of smallest to largest\n",
        "        # We take the last 5, and then reverse them [::-1]\n",
        "        top_5_indices = np.argsort(np.abs(component))[-5:][::-1]\n",
        "        \n",
        "        # Print the feature name and its loading (weight)\n",
        "        for j, feature_index in enumerate(top_5_indices):\n",
        "            feature_name = feature_names[feature_index]\n",
        "            loading = component[feature_index]\n",
        "            print(f\"    {j+1}. {feature_name}: {loading:.4f}\")\n",
        "            \n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # --- End of report ---\n",
        "    \n",
        "    # Get the list of explained variances\n",
        "    explained_variance_list = pca.explained_variance_ratio_.tolist()\n",
        "    \n",
        "    # Return the new data, the fitted PCA object, and the list of variances\n",
        "    return data_transformed, pca, explained_variance_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dddeee11",
      "metadata": {},
      "source": [
        "# Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a61a8515",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Get data to be a numpy array probably\n",
        "df=pd.read_csv('consolidated_acs_data_clean.csv')\n",
        "data=np.array(df)\n",
        "income_index=1  # Example index for income dimension\n",
        "# Transform the income dimension by logarithmic scale\n",
        "data[:, income_index] = np.log(data[:, income_index].astype(np.float64))\n",
        "# Normalize data by dividing by standard deviation by dimension\n",
        "for i in range(1,data.shape[1]):\n",
        "    data[:, i] = data[:, i] / np.std(data[:, i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b78064e",
      "metadata": {},
      "source": [
        "## Weighting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "40bdd910",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Initilize weights, and scale data by weights\n",
        "# Get column indices\n",
        "income_index = 1  # per_capita_income\n",
        "poverty_index = df.columns.get_loc('avg_poverty_ratio')\n",
        "employment_index = df.columns.get_loc('in_labor_force')\n",
        "healthcare_index = df.columns.get_loc('Educational services, and health care and social assistance')\n",
        "education_index = df.columns.get_loc('College or More')\n",
        "commute_index = df.columns.get_loc('avg_commute_time')\n",
        "housing_index = df.columns.get_loc('avg_housing_cost_burden')\n",
        "race_indices = [df.columns.get_loc(col) for col in ['white', 'black', 'asian', 'native', 'pacific islander', 'other']]\n",
        "\n",
        "# Apply dimension weights\n",
        "data[:, income_index] *= 0.1225          # Economic Security - Income\n",
        "data[:, poverty_index] *= 0.0525         # Economic Security - Poverty\n",
        "data[:, employment_index] *= 0.07        # Economic Security - Employment\n",
        "data[:, healthcare_index] *= 0.105       # Economic Security - Healthcare\n",
        "data[:, education_index] *= 0.15         # Education\n",
        "data[:, commute_index] *= 0.15           # Location Affordability - Transportation\n",
        "data[:, housing_index] *= 0.15           # Location Affordability - Housing\n",
        "\n",
        "# Race: distribute 0.2 across 6 race columns\n",
        "for idx in race_indices:\n",
        "    data[:, idx] *= 0.2 / 6  # 0.0333 per column\n",
        "\n",
        "weights = np.array([\n",
        "    0.1225,     # 1:  per_capita_income (Economic Security - Income)\n",
        "    0.2/6,      # 2:  white (Cultural - Race)\n",
        "    0.2/6,      # 3:  black (Cultural - Race)\n",
        "    0.2/6,      # 4:  asian (Cultural - Race)\n",
        "    0.2/6,      # 5:  native (Cultural - Race)\n",
        "    0.2/6,      # 6:  pacific islander (Cultural - Race)\n",
        "    0.2/6,      # 7:  other (Cultural - Race)\n",
        "    0.0,        # 8:  Under High School (not weighted)\n",
        "    0.0,        # 9:  High School (No College Degree) (not weighted)\n",
        "    0.15,       # 10: College or More (Education)\n",
        "    0.0,        # 11: Agriculture (not weighted)\n",
        "    0.0,        # 12: Construction_and_manufacturing (not weighted)\n",
        "    0.0,        # 13: trade (not weighted)\n",
        "    0.0,        # 14: Transportation and warehousing (not weighted)\n",
        "    0.0,        # 15: nerds (not weighted)\n",
        "    0.105,      # 16: Educational services, and health care (Economic Security - Healthcare)\n",
        "    0.0,        # 17: finance_inurance_and_realty (not weighted)\n",
        "    0.0,        # 18: other_services (not weighted)\n",
        "    0.07,       # 19: in_labor_force (Economic Security - Employment)\n",
        "    0.0,        # 20: out_labor_force (not weighted)\n",
        "    0.15,       # 21: avg_commute_time (Location Affordability - Transportation)\n",
        "    0.15,       # 22: avg_housing_cost_burden (Location Affordability - Housing)\n",
        "    0.0525,     # 23: avg_poverty_ratio (Economic Security - Poverty)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2d4f928a",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "PCA Dimensionality Reduction Report\n",
            "------------------------------\n",
            "Original dimensions:   23\n",
            "New dimensions:        6\n",
            "Dimensions reduced by: 17\n",
            "\n",
            "Variance explained by each remaining component:\n",
            "  Principal Component 1: 16.07%\n",
            "  Principal Component 2: 10.11%\n",
            "  Principal Component 3: 8.43%\n",
            "  Principal Component 4: 6.79%\n",
            "  Principal Component 5: 6.32%\n",
            "  Principal Component 6: 5.67%\n",
            "\n",
            "Total variance explained: 53.38%\n",
            "(Target threshold was 50%)\n",
            "\n",
            "Top 5 Contributors per Component:\n",
            "  --- Principal Component 1 ---\n",
            "    1. College or More: 0.4448\n",
            "    2. avg_poverty_ratio: 0.3872\n",
            "    3. Under High School: -0.3443\n",
            "    4. per_capita_income: 0.2592\n",
            "    5. in_labor_force: 0.2370\n",
            "  --- Principal Component 2 ---\n",
            "    1. out_labor_force: -0.4101\n",
            "    2. in_labor_force: 0.4101\n",
            "    3. white: -0.3803\n",
            "    4. pacific islander: -0.3752\n",
            "    5. Agriculture, forestry, fishing and hunting, and mining: -0.2772\n",
            "  --- Principal Component 3 ---\n",
            "    1. High School (No College Degree): 0.4196\n",
            "    2. avg_housing_cost_burden: -0.4130\n",
            "    3. white: 0.3610\n",
            "    4. Construction_and_manufacturing: 0.3370\n",
            "    5. asian: -0.2544\n",
            "  --- Principal Component 4 ---\n",
            "    1. other: 0.4171\n",
            "    2. Under High School: 0.3589\n",
            "    3. black: -0.3423\n",
            "    4. Educational services, and health care and social assistance: -0.2938\n",
            "    5. out_labor_force: -0.2873\n",
            "  --- Principal Component 5 ---\n",
            "    1. other: 0.4635\n",
            "    2. out_labor_force: 0.4026\n",
            "    3. in_labor_force: -0.4026\n",
            "    4. pacific islander: -0.4001\n",
            "    5. native: -0.2099\n",
            "  --- Principal Component 6 ---\n",
            "    1. other_services: 0.6451\n",
            "    2. black: -0.3944\n",
            "    3. avg_commute_time: -0.3466\n",
            "    4. Transportation and warehousing, and utilities: -0.3019\n",
            "    5. white: 0.2319\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Perform PCA, and project onto the top N dimensions so that they explain 50% of the variance\n",
        "new_data,pca,var_explained = reduce_pca_by_variance(data[:,1:], list(df.columns)[1:], variance_threshold=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deb9dd36",
      "metadata": {},
      "source": [
        "As shown, the first component has a distinct identity, namely education, income and poverty, part of the most important factors typically considered as determinant of a person's sociol-economic status. The rest of the components are a mixed of race, education, occupation, and comute time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e351f034",
      "metadata": {},
      "source": [
        "# Network initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "84b89226",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_dimension_layered_knn(data, dimension_weights, k=10):\n",
        "    \"\"\"\n",
        "    Create multi-layer network where each dimension has its own KNN graph.\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "    \n",
        "    # Add nodes\n",
        "    for i in range(data.shape[0]):\n",
        "        G.add_node(i)\n",
        "    \n",
        "    # For each dimension, create KNN graph\n",
        "    for dim in range(data.shape[1]):\n",
        "        dim_weight = dimension_weights[dim]\n",
        "        \n",
        "        if dim_weight == 0:\n",
        "            continue\n",
        "        \n",
        "        # Get this dimension's values (1D)\n",
        "        dim_data = data[:, dim].reshape(-1, 1)\n",
        "        \n",
        "        # Build KNN graph for THIS dimension only\n",
        "        from sklearn.neighbors import NearestNeighbors\n",
        "        nbrs = NearestNeighbors(n_neighbors=k)\n",
        "        nbrs.fit(dim_data)\n",
        "        distances, indices = nbrs.kneighbors(dim_data)\n",
        "        \n",
        "        # Add edges weighted by dimension importance\n",
        "        for i in range(len(data)):\n",
        "            for j, neighbor in enumerate(indices[i]):\n",
        "                if i != neighbor:\n",
        "                    if G.has_edge(i, neighbor):\n",
        "                        G[i][neighbor]['weight'] += dim_weight  # Accumulate\n",
        "                    else:\n",
        "                        G.add_edge(i, neighbor, weight=dim_weight)\n",
        "    \n",
        "    return G\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "eba930f8",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# from ring_network import construct_net\n",
        "G = create_dimension_layered_knn(new_data, dimension_weights=var_explained, k=10)\n",
        "G_uni= nx.Graph()\n",
        "G_uni.add_nodes_from(G.nodes(data=True))\n",
        "G_uni.add_edges_from(G.edges())\n",
        "\n",
        "# Gring= construct_net(new_data,var_explained,0.2)\n",
        "# Gring_uni= nx.Graph()\n",
        "# Gring_uni.add_nodes_from(Gring.nodes(data=True))\n",
        "# Gring_uni.add_edges_from(Gring.edges())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7250c6c9",
      "metadata": {},
      "source": [
        "## Geographic weighting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a1161760",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from scipy.sparse import load_npz\n",
        "\n",
        "# Create graph\n",
        "Geo = G.copy()\n",
        "#Gring_geo=Gring.copy()\n",
        "# Load adjacency matrix\n",
        "adj_matrix = load_npz('adjacency_queen_matrix.npz')\n",
        "\n",
        "# Load mappings\n",
        "with open('adjacency_queen_mappings.pkl', 'rb') as f:\n",
        "    mappings = pickle.load(f)\n",
        "index_to_geoid = mappings['index_to_geoid']\n",
        "\n",
        "# Get edges from sparse matrix\n",
        "rows, cols = adj_matrix.nonzero()\n",
        "\n",
        "# Add edges with your weight\n",
        "weight = 2  # Change this to your desired weight\n",
        "\n",
        "for i, j in zip(rows, cols):\n",
        "    if i < j:  # Only add each edge once (undirected)\n",
        "        geoid1 = index_to_geoid[i]\n",
        "        geoid2 = index_to_geoid[j]\n",
        "        Geo.add_edge(i, j, weight=weight)\n",
        "        #Gring_geo.add_edge(i, j, weight=weight)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2768f93b",
      "metadata": {},
      "source": [
        "# Grouping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4cc3c755",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# K means clustering function\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def get_kmeans_partition(data: np.ndarray, weights, n_clusters=14):\n",
        "    \"\"\"\n",
        "    Runs K-means clustering on the input data and returns the loss\n",
        "    (inertia) and a partition of the data indices by cluster.\n",
        "\n",
        "    Args:\n",
        "        data: A (n_samples, n_features) NumPy array.\n",
        "        n_clusters: The number of clusters (k).\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - loss (float): The inertia (Within-Cluster Sum of Squares).\n",
        "        - partitions (dict): A dictionary where keys are cluster IDs (0 to k-1)\n",
        "                             and values are lists of original data indices\n",
        "                             belonging to that cluster.\n",
        "    \"\"\"\n",
        "    \n",
        "    # 1. Initialize and fit the K-means model\n",
        "    # n_init=10 runs the algorithm 10 times and picks the best result\n",
        "    # random_state=42 ensures the result is reproducible\n",
        "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
        "    for i in range(len(weights)):\n",
        "        data[:, i] = data[:, i] * weights[i]\n",
        "    kmeans.fit(data)\n",
        "\n",
        "    # 2. Get the loss (inertia)\n",
        "    # .inertia_ is the WCSS (Within-Cluster Sum of Squares)\n",
        "    loss = kmeans.inertia_\n",
        "\n",
        "    # 3. Get the cluster assignment for each data point\n",
        "    # .labels_ is an array like [0, 1, 1, 0, 2, ...]\n",
        "    labels = kmeans.labels_\n",
        "\n",
        "    # 4. Create the partition of indices\n",
        "    partitions = {i: [] for i in range(n_clusters)}\n",
        "    for index, cluster_id in enumerate(labels):\n",
        "        partitions[cluster_id].append(index)\n",
        "\n",
        "    return loss, partitions\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "def calculate_kmeans_loss_for_partition(data: np.ndarray, partition: list) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the K-means \"loss\" (Inertia, or Within-Cluster Sum of Squares)\n",
        "    for a given dataset and a user-provided partition.\n",
        "\n",
        "    Args:\n",
        "        data: A (n_samples, n_features) NumPy array.\n",
        "        partition: A list of lists, where each inner list contains the\n",
        "                   *indices* (row numbers) of the data points belonging\n",
        "                   to that cluster.\n",
        "                   Example: [[0, 1, 4], [2, 3, 5]]\n",
        "\n",
        "    Returns:\n",
        "        total_loss (float): The total K-means loss (Inertia) for this partition.\n",
        "    \"\"\"\n",
        "    \n",
        "    total_loss = 0.0\n",
        "\n",
        "    # Iterate over each cluster (which is a list of indices)\n",
        "    for indices in partition:\n",
        "        \n",
        "        # 1. Get all data points belonging to this cluster\n",
        "        # Using [indices, :] selects all rows whose index is in the list\n",
        "        cluster_points = data[indices, :]\n",
        "        \n",
        "        # Handle empty clusters (their loss is 0)\n",
        "        if cluster_points.shape[0] == 0:\n",
        "            continue\n",
        "            \n",
        "        # 2. Calculate the \"true\" centroid (mean) for this cluster\n",
        "        # axis=0 calculates the mean of each *column* (feature)\n",
        "        centroid = np.mean(cluster_points, axis=0)\n",
        "        \n",
        "        # 3. Calculate the sum of squared distances from each point to the centroid\n",
        "        #    - (cluster_points - centroid) uses broadcasting to get distance vectors\n",
        "        #    - (** 2) squares all distances\n",
        "        #    - np.sum(...) sums all squared distances into a single number\n",
        "        cluster_loss = np.sum((cluster_points - centroid) ** 2)\n",
        "        \n",
        "        # 4. Add this cluster's loss to the total\n",
        "        total_loss += cluster_loss\n",
        "        \n",
        "    return total_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "k = 14\n",
        "\n",
        "# 3. Run the function\n",
        "total_loss, index_partitions = get_kmeans_partition(new_data,weights=var_explained, n_clusters=k)\n",
        "\n",
        "index_partitions=[index_partitions[i] for i in range(k)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "463ea6ed",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Louvain communities G: 16\n",
            "Number of Louvain communities Geo: 22\n",
            "Number of Louvain communities G_uni: 18\n"
          ]
        }
      ],
      "source": [
        "partG=nx.algorithms.community.louvain_communities(G,weight='weight',resolution=0.75)\n",
        "print(\"Number of Louvain communities G:\",len(partG))\n",
        "partGeo=nx.algorithms.community.louvain_communities(Geo,weight='weight',resolution=0.8)\n",
        "print(\"Number of Louvain communities Geo:\",len(partGeo))\n",
        "partG_uni=nx.algorithms.community.louvain_communities(G_uni,weight='weight',resolution=0.95)\n",
        "print(\"Number of Louvain communities G_uni:\",len(partG_uni))\n",
        "\n",
        "# partGring=nx.algorithms.community.louvain_communities(Gring,weight='weight',resolution=0.65)\n",
        "# print(\"Number of Louvain communities ring:\",len(partGring))\n",
        "# partRing_geo=nx.algorithms.community.louvain_communities(Gring_geo,weight='weight',resolution=0.7)\n",
        "# print(\"Number of Louvain communities ring geo:\",len(partRing_geo))\n",
        "# partRing_uni=nx.algorithms.community.louvain_communities(Gring_uni,resolution=0.75)\n",
        "# print(\"Number of Louvain communities ring uni:\",len(partRing_uni))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3d6c94e4",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "152.14375134845392\n"
          ]
        }
      ],
      "source": [
        "print(np.std([len(x) for x in partGeo]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "600c8e70",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " 10Graph Comparisons\n",
            "Louvain modularity G: 0.2846734056774947\n",
            "Louvain modularity Geo partition on G: 0.5163148151988096\n",
            "Louvain modularity Uni partition on G: 0.15494924304467428\n",
            "\n",
            " K Means Comparisons\n",
            "K means modularity on G: 0.10560605648755782\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n 10Graph Comparisons\")\n",
        "print(\"Louvain modularity G:\",nx.algorithms.community.modularity(G, partG, weight='weight'))\n",
        "print(\"Louvain modularity Geo partition on G:\",nx.algorithms.community.modularity(Geo, partGeo, weight='weight'))\n",
        "print(\"Louvain modularity Uni partition on G:\",nx.algorithms.community.modularity(G, partG_uni, weight='weight'))\n",
        "\n",
        "# print(\"\\n Ring Comparisons\")\n",
        "# print(\"Louvain modularity ring:\",nx.algorithms.community.modularity(Gring, partGring, weight='weight'))\n",
        "# print(\"Louvain modularity Geo partition on ring:\",nx.algorithms.community.modularity(Gring, partRing_geo, weight='weight'))\n",
        "# print(\"Louvain modularity Uni partition on Gring:\",nx.algorithms.community.modularity(Gring, partRing_uni, weight='weight'))\n",
        "\n",
        "print(\"\\n K Means Comparisons\")\n",
        "print(\"K means modularity on G:\",nx.algorithms.community.modularity(G, index_partitions, weight='weight'))\n",
        "# print(\"K means modularity on ring:\",nx.algorithms.community.modularity(Gring, index_partitions, weight='weight'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "57f9fb8d",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Load your real data (as you were before)\n",
        "\n",
        "# 2. Get your partition\n",
        "# (e.g., from K-means, Louvain, or any other method)\n",
        "# This is just a placeholder example!\n",
        "# You would have your *real* partition here.\n",
        "\n",
        "# 3. Set the path to the shapefile you downloaded\n",
        "# (Update this path to where you unzipped the file)\n",
        "SHAPEFILE = \"tl_2020_37_tract/tl_2020_37_tract.shp\"\n",
        "\n",
        "# 4. Call the plotting function\n",
        "# from plot_geo import *\n",
        "# plot_partition_geographically(data, partGeo, SHAPEFILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e872f88b",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# color=[0]*data.shape[0]\n",
        "# for i in range(len(partition)):\n",
        "#     for o in partition[i]:\n",
        "#         color[o]=i\n",
        "# nx.draw(G,node_color=color,node_size=1)\n",
        "# pos = nx.spring_layout(G, seed=42)\n",
        "# nx.draw_networkx_nodes(G, pos, node_color=color, node_size=1)\n",
        "# nx.draw_networkx_edges(G, pos, edge_color='black', alpha=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "13cf2321-ddc4-4aa4-8f04-076eebe254b4",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# need initial partition to have 14 groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4d59835f-38f6-4dc7-abdb-1d9f77968e02",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from typing import List, Set, Dict, Tuple\n",
        "import time\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def partition_to_map(partition: List[Set[int]]) -> Dict[int, int]:\n",
        "    \"\"\"Converts a list of sets to a dictionary mapping node -> community_index.\"\"\"\n",
        "    node_to_community = {}\n",
        "    for i, community_set in enumerate(partition):\n",
        "        for node in community_set:\n",
        "            node_to_community[node] = i\n",
        "    return node_to_community\n",
        "\n",
        "def check_contiguity(Geo: nx.Graph, old_community_idx: int, prospective_partition: List[Set[int]]) -> bool:\n",
        "    \"\"\"\n",
        "    Checks if the community at old_community_idx remains contiguous after a move.\n",
        "    \n",
        "    Args:\n",
        "        Geo: The geographical adjacency graph.\n",
        "        old_community_idx: The index of the community to check.\n",
        "        prospective_partition: The partition *after* the move has been simulated.\n",
        "        \n",
        "    Returns:\n",
        "        True if the community remains contiguous, False otherwise.\n",
        "    \"\"\"\n",
        "    \n",
        "    remaining_nodes = prospective_partition[old_community_idx]\n",
        "    \n",
        "    if len(remaining_nodes) <= 1:\n",
        "        return True\n",
        "        \n",
        "    # Get the first node to start the check\n",
        "    start_node = next(iter(remaining_nodes))\n",
        "    \n",
        "    # Perform a graph traversal (BFS) on the subgraph induced by remaining_nodes\n",
        "    # use the Geo graph's edges for contiguity.\n",
        "    \n",
        "    # Create a set of reachable nodes\n",
        "    reachable = set()\n",
        "    queue = [start_node]\n",
        "    \n",
        "    while queue:\n",
        "        current = queue.pop(0)\n",
        "        if current not in reachable:\n",
        "            reachable.add(current)\n",
        "            # Check neighbors in the Geo graph\n",
        "            for neighbor in Geo.neighbors(current):\n",
        "                # Only follow edges where the neighbor is still in the community\n",
        "                if neighbor in remaining_nodes and neighbor not in reachable:\n",
        "                    queue.append(neighbor)\n",
        "                    \n",
        "    # Check if all remaining nodes were reached\n",
        "    return len(reachable) == len(remaining_nodes)\n",
        "\n",
        "# --- Core Optimization Function with Sequential Swap and Delta Q ---\n",
        "def optimized_sequential_swap(G: nx.Graph, Geo: nx.Graph, initial_partition: List[Set[int]], N_iterations: int) -> Tuple[List[Set[int]], float]:\n",
        "    \"\"\"\n",
        "    Performs local search optimization using a sequential, targeted two-node swap.\n",
        "    In each iteration, it finds the single best swap across all districts and executes it.\n",
        "    \"\"\"\n",
        "    \n",
        "    # 1. Pre-calculate initial community properties\n",
        "    best_part = [set(comm) for comm in initial_partition] # current best partition\n",
        "    node_to_community_map = partition_to_map(best_part) # convert to map for better searching\n",
        "    \n",
        "    # Total edge weight of the graph G (2m)\n",
        "    total_weight_2m = G.size(weight='weight') * 2\n",
        "    \n",
        "    # Calculate initial community properties for Delta Q calculation (change in modularity)\n",
        "    community_properties = {}\n",
        "    for i, community in enumerate(best_part):\n",
        "        subgraph = G.subgraph(community)\n",
        "        total_internal_weight = subgraph.size(weight='weight')\n",
        "        total_degree_sum = sum(G.degree(node, weight='weight') for node in community)\n",
        "        \n",
        "        community_properties[i] = {\n",
        "            'internal_weight': total_internal_weight,\n",
        "            'degree_sum': total_degree_sum\n",
        "        }\n",
        "    \n",
        "    # Calculate initial modularity score\n",
        "    modularity_part = tuple(frozenset(s) for s in best_part if s)\n",
        "    best_score = nx.algorithms.community.modularity(G, modularity_part, weight='weight')\n",
        "\n",
        "    total_duration = 0.0\n",
        "    \n",
        "    print(f\"Starting optimization with Modularity Score: {best_score:.4f}\")\n",
        "    \n",
        "    # --- Main Optimization Loop ---\n",
        "    for i in range(N_iterations):\n",
        "        start_time = time.time()\n",
        "        print(f\"--- Iteration {i+1}/{N_iterations} ---\")\n",
        "        \n",
        "        best_gain = 0.0\n",
        "        best_swap = None  # Stores: (node_A, node_B, comm_A_idx, comm_B_idx)\n",
        "        \n",
        "        # 2. Iterate through each district (community)\n",
        "        # We use a list of indices to ensure we iterate over all districts\n",
        "        district_indices = list(range(len(best_part)))\n",
        "        \n",
        "        # 3. Find the single best swap across ALL districts\n",
        "        for comm_A_idx in district_indices:\n",
        "            \n",
        "            # Get all nodes in the current district\n",
        "            district_A_nodes = best_part[comm_A_idx]\n",
        "            \n",
        "            # Identify all potential swap candidates involving a node from comm_A_idx\n",
        "            # A swap candidate is a pair (A, B) where A is in comm_A_idx and B is adjacent to A\n",
        "            # and B is in a different community comm_B_idx.\n",
        "            \n",
        "            # Iterate over all nodes A in the current district\n",
        "            for node_A in district_A_nodes:\n",
        "                \n",
        "                # Iterate over all geographical neighbors of A\n",
        "                for node_B in Geo.neighbors(node_A):\n",
        "                    \n",
        "                    comm_B_idx = node_to_community_map.get(node_B)\n",
        "                    \n",
        "                    # Check if B is in a different community\n",
        "                    if comm_B_idx is not None and comm_B_idx != comm_A_idx:\n",
        "                        \n",
        "                        # Delta Q change in modularity calculation\n",
        "                        \n",
        "                        # 1. Calculate gain for moving A from comm_A to comm_B\n",
        "                        k_A = G.degree(node_A, weight='weight')\n",
        "                        k_A_in_B = sum(G[node_A][neighbor]['weight'] \n",
        "                                       for neighbor in G.neighbors(node_A) \n",
        "                                       if node_to_community_map.get(neighbor) == comm_B_idx)\n",
        "                        \n",
        "                        sigma_tot_A = community_properties[comm_A_idx]['degree_sum']\n",
        "                        sigma_tot_B = community_properties[comm_B_idx]['degree_sum']\n",
        "                        \n",
        "                        # Delta Q for A moving to B (ignoring B)\n",
        "                        gain_A = (k_A_in_B / total_weight_2m) - (k_A * sigma_tot_B / (total_weight_2m ** 2))\n",
        "                        \n",
        "                        # 2. Calculate gain for moving B from comm_B to comm_A\n",
        "                        k_B = G.degree(node_B, weight='weight')\n",
        "                        k_B_in_A = sum(G[node_B][neighbor]['weight'] \n",
        "                                       for neighbor in G.neighbors(node_B) \n",
        "                                       if node_to_community_map.get(neighbor) == comm_A_idx)\n",
        "                        \n",
        "                        # Delta Q for B moving to A (ignoring A)\n",
        "                        gain_B = (k_B_in_A / total_weight_2m) - (k_B * sigma_tot_A / (total_weight_2m ** 2))\n",
        "                        \n",
        "                        # 3. Total Swap Gain (Sum of individual gains + correction)\n",
        "                        w_AB = G.get_edge_data(node_A, node_B, default={'weight': 0.0})['weight']\n",
        "                        total_gain = gain_A + gain_B + (w_AB / total_weight_2m)\n",
        "                        \n",
        "                        # --- B. Contiguity Check  ---\n",
        "                        if total_gain > best_gain:\n",
        "                            # Simulate the swap for the contiguity check\n",
        "                            prospective_part = [set(comm) for comm in best_part]\n",
        "                            \n",
        "                            # A leaves comm_A, B leaves comm_B\n",
        "                            prospective_part[comm_A_idx].remove(node_A)\n",
        "                            prospective_part[comm_B_idx].remove(node_B)\n",
        "                            \n",
        "                            # A enters comm_B, B enters comm_A\n",
        "                            prospective_part[comm_B_idx].add(node_A)\n",
        "                            prospective_part[comm_A_idx].add(node_B)\n",
        "                            \n",
        "                            # Check if BOTH communities remain contiguous\n",
        "                            contiguity_A = check_contiguity(Geo, comm_A_idx, prospective_part)\n",
        "                            contiguity_B = check_contiguity(Geo, comm_B_idx, prospective_part)\n",
        "                            \n",
        "                            if contiguity_A and contiguity_B:\n",
        "                                # 4. If the gain is the best so far, record the swap\n",
        "                                best_gain = total_gain\n",
        "                                best_swap = (node_A, node_B, comm_A_idx, comm_B_idx)\n",
        "\n",
        "        # 5. Apply the single best swap for this iteration\n",
        "        if best_gain > 0:\n",
        "            node_A, node_B, comm_A_idx, comm_B_idx = best_swap\n",
        "            \n",
        "            # Update partition sets\n",
        "            best_part[comm_A_idx].remove(node_A)\n",
        "            best_part[comm_B_idx].remove(node_B)\n",
        "            best_part[comm_B_idx].add(node_A)\n",
        "            best_part[comm_A_idx].add(node_B)\n",
        "            \n",
        "            # Update node-to-community map\n",
        "            node_to_community_map[node_A] = comm_B_idx\n",
        "            node_to_community_map[node_B] = comm_A_idx\n",
        "            \n",
        "            # Re-calculate properties for the two affected communities\n",
        "            for idx in [comm_A_idx, comm_B_idx]:\n",
        "                community = best_part[idx]\n",
        "                subgraph = G.subgraph(community)\n",
        "                community_properties[idx]['internal_weight'] = subgraph.size(weight='weight')\n",
        "                community_properties[idx]['degree_sum'] = sum(G.degree(node, weight='weight') for node in community)\n",
        "            \n",
        "            # Update total modularity score\n",
        "            best_score += best_gain\n",
        "            \n",
        "            print(f\"  Swap accepted! Node {node_A} moved from {comm_A_idx} to {comm_B_idx}, and Node {node_B} moved from {comm_B_idx} to {comm_A_idx}.\")\n",
        "            print(f\"  Modularity increased by: {best_gain:.6f}. New score: {best_score:.4f}\")\n",
        "            \n",
        "        else:\n",
        "            print(\"\\nNo local swap increased modularity while maintaining contiguity. Optimization finished.\")\n",
        "            break\n",
        "            \n",
        "        end_time = time.time()\n",
        "        duration = end_time - start_time\n",
        "        print(f\"  Iteration {i+1} took: {duration:.2f} seconds\")\n",
        "        total_duration += duration\n",
        "            \n",
        "    print(\"\\n--- Final Results ---\")\n",
        "    print(f\"Optimal Modularity Score (after local swaps): {best_score:.4f}\")\n",
        "    return best_part, best_score, total_duration\n",
        "\n",
        "# --- Example Usage (Requires G, Geo, and partGeo to be defined in the notebook) ---\n",
        "# best_partition, final_score = optimized_local_search(G, Geo, partGeo, N_iterations=100)\n",
        "# print(f\"Final Partition Size: {len(best_partition)}\")\n",
        "# print(f\"Final Modularity Score: {final_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5841fb6b-18d5-4127-aba1-962727386c81",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting optimization with Modularity Score: 0.0107\n",
            "--- Iteration 1/100 ---\n",
            "  Swap accepted! Node 1051 moved from 7 to 13, and Node 3832 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000056. New score: 0.0108\n",
            "  Iteration 1 took: 15.16 seconds\n",
            "--- Iteration 2/100 ---\n",
            "  Swap accepted! Node 1950 moved from 7 to 13, and Node 3827 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000055. New score: 0.0108\n",
            "  Iteration 2 took: 14.94 seconds\n",
            "--- Iteration 3/100 ---\n",
            "  Swap accepted! Node 1994 moved from 7 to 20, and Node 6199 moved from 20 to 7.\n",
            "  Modularity increased by: 0.000054. New score: 0.0109\n",
            "  Iteration 3 took: 14.96 seconds\n",
            "--- Iteration 4/100 ---\n",
            "  Swap accepted! Node 6494 moved from 7 to 13, and Node 4023 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000054. New score: 0.0110\n",
            "  Iteration 4 took: 15.05 seconds\n",
            "--- Iteration 5/100 ---\n",
            "  Swap accepted! Node 4023 moved from 7 to 13, and Node 4012 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000060. New score: 0.0110\n",
            "  Iteration 5 took: 15.14 seconds\n",
            "--- Iteration 6/100 ---\n",
            "  Swap accepted! Node 4405 moved from 2 to 7, and Node 4876 moved from 7 to 2.\n",
            "  Modularity increased by: 0.000055. New score: 0.0111\n",
            "  Iteration 6 took: 15.28 seconds\n",
            "--- Iteration 7/100 ---\n",
            "  Swap accepted! Node 538 moved from 1 to 7, and Node 1904 moved from 7 to 1.\n",
            "  Modularity increased by: 0.000053. New score: 0.0111\n",
            "  Iteration 7 took: 14.83 seconds\n",
            "--- Iteration 8/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0112\n",
            "  Iteration 8 took: 14.90 seconds\n",
            "--- Iteration 9/100 ---\n",
            "  Swap accepted! Node 6429 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000051. New score: 0.0112\n",
            "  Iteration 9 took: 15.83 seconds\n",
            "--- Iteration 10/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3867 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000056. New score: 0.0113\n",
            "  Iteration 10 took: 16.02 seconds\n",
            "--- Iteration 11/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0113\n",
            "  Iteration 11 took: 14.82 seconds\n",
            "--- Iteration 12/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0114\n",
            "  Iteration 12 took: 14.13 seconds\n",
            "--- Iteration 13/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0114\n",
            "  Iteration 13 took: 15.21 seconds\n",
            "--- Iteration 14/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0115\n",
            "  Iteration 14 took: 16.70 seconds\n",
            "--- Iteration 15/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0115\n",
            "  Iteration 15 took: 15.50 seconds\n",
            "--- Iteration 16/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0116\n",
            "  Iteration 16 took: 15.38 seconds\n",
            "--- Iteration 17/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0116\n",
            "  Iteration 17 took: 14.94 seconds\n",
            "--- Iteration 18/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0117\n",
            "  Iteration 18 took: 14.60 seconds\n",
            "--- Iteration 19/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0117\n",
            "  Iteration 19 took: 15.39 seconds\n",
            "--- Iteration 20/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0118\n",
            "  Iteration 20 took: 15.23 seconds\n",
            "--- Iteration 21/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0118\n",
            "  Iteration 21 took: 14.26 seconds\n",
            "--- Iteration 22/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0119\n",
            "  Iteration 22 took: 15.45 seconds\n",
            "--- Iteration 23/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0119\n",
            "  Iteration 23 took: 15.95 seconds\n",
            "--- Iteration 24/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0120\n",
            "  Iteration 24 took: 14.92 seconds\n",
            "--- Iteration 25/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0120\n",
            "  Iteration 25 took: 15.27 seconds\n",
            "--- Iteration 26/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0121\n",
            "  Iteration 26 took: 15.62 seconds\n",
            "--- Iteration 27/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0121\n",
            "  Iteration 27 took: 15.59 seconds\n",
            "--- Iteration 28/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0122\n",
            "  Iteration 28 took: 15.49 seconds\n",
            "--- Iteration 29/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0123\n",
            "  Iteration 29 took: 16.14 seconds\n",
            "--- Iteration 30/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0123\n",
            "  Iteration 30 took: 16.49 seconds\n",
            "--- Iteration 31/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0124\n",
            "  Iteration 31 took: 15.60 seconds\n",
            "--- Iteration 32/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0124\n",
            "  Iteration 32 took: 15.82 seconds\n",
            "--- Iteration 33/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0125\n",
            "  Iteration 33 took: 15.26 seconds\n",
            "--- Iteration 34/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0125\n",
            "  Iteration 34 took: 16.10 seconds\n",
            "--- Iteration 35/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0126\n",
            "  Iteration 35 took: 16.07 seconds\n",
            "--- Iteration 36/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0126\n",
            "  Iteration 36 took: 15.56 seconds\n",
            "--- Iteration 37/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0127\n",
            "  Iteration 37 took: 16.05 seconds\n",
            "--- Iteration 38/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0127\n",
            "  Iteration 38 took: 16.06 seconds\n",
            "--- Iteration 39/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0128\n",
            "  Iteration 39 took: 15.15 seconds\n",
            "--- Iteration 40/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0128\n",
            "  Iteration 40 took: 15.82 seconds\n",
            "--- Iteration 41/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0129\n",
            "  Iteration 41 took: 15.43 seconds\n",
            "--- Iteration 42/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0129\n",
            "  Iteration 42 took: 15.54 seconds\n",
            "--- Iteration 43/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0130\n",
            "  Iteration 43 took: 15.73 seconds\n",
            "--- Iteration 44/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0130\n",
            "  Iteration 44 took: 15.43 seconds\n",
            "--- Iteration 45/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0131\n",
            "  Iteration 45 took: 16.39 seconds\n",
            "--- Iteration 46/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0131\n",
            "  Iteration 46 took: 16.55 seconds\n",
            "--- Iteration 47/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0132\n",
            "  Iteration 47 took: 15.88 seconds\n",
            "--- Iteration 48/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0132\n",
            "  Iteration 48 took: 16.07 seconds\n",
            "--- Iteration 49/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0133\n",
            "  Iteration 49 took: 15.81 seconds\n",
            "--- Iteration 50/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0133\n",
            "  Iteration 50 took: 16.08 seconds\n",
            "--- Iteration 51/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0134\n",
            "  Iteration 51 took: 15.96 seconds\n",
            "--- Iteration 52/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0134\n",
            "  Iteration 52 took: 15.90 seconds\n",
            "--- Iteration 53/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0135\n",
            "  Iteration 53 took: 15.64 seconds\n",
            "--- Iteration 54/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0135\n",
            "  Iteration 54 took: 15.79 seconds\n",
            "--- Iteration 55/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0136\n",
            "  Iteration 55 took: 15.69 seconds\n",
            "--- Iteration 56/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0136\n",
            "  Iteration 56 took: 15.69 seconds\n",
            "--- Iteration 57/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0137\n",
            "  Iteration 57 took: 16.30 seconds\n",
            "--- Iteration 58/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0137\n",
            "  Iteration 58 took: 15.79 seconds\n",
            "--- Iteration 59/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0138\n",
            "  Iteration 59 took: 15.85 seconds\n",
            "--- Iteration 60/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0138\n",
            "  Iteration 60 took: 15.59 seconds\n",
            "--- Iteration 61/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0139\n",
            "  Iteration 61 took: 15.74 seconds\n",
            "--- Iteration 62/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0139\n",
            "  Iteration 62 took: 15.48 seconds\n",
            "--- Iteration 63/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0140\n",
            "  Iteration 63 took: 15.40 seconds\n",
            "--- Iteration 64/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0140\n",
            "  Iteration 64 took: 16.19 seconds\n",
            "--- Iteration 65/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0141\n",
            "  Iteration 65 took: 15.74 seconds\n",
            "--- Iteration 66/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0141\n",
            "  Iteration 66 took: 16.01 seconds\n",
            "--- Iteration 67/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0142\n",
            "  Iteration 67 took: 16.06 seconds\n",
            "--- Iteration 68/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0142\n",
            "  Iteration 68 took: 15.84 seconds\n",
            "--- Iteration 69/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0143\n",
            "  Iteration 69 took: 16.02 seconds\n",
            "--- Iteration 70/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0143\n",
            "  Iteration 70 took: 16.18 seconds\n",
            "--- Iteration 71/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0144\n",
            "  Iteration 71 took: 16.06 seconds\n",
            "--- Iteration 72/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0144\n",
            "  Iteration 72 took: 15.05 seconds\n",
            "--- Iteration 73/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0145\n",
            "  Iteration 73 took: 15.78 seconds\n",
            "--- Iteration 74/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0145\n",
            "  Iteration 74 took: 15.88 seconds\n",
            "--- Iteration 75/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0146\n",
            "  Iteration 75 took: 16.23 seconds\n",
            "--- Iteration 76/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0147\n",
            "  Iteration 76 took: 16.30 seconds\n",
            "--- Iteration 77/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0147\n",
            "  Iteration 77 took: 15.88 seconds\n",
            "--- Iteration 78/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0148\n",
            "  Iteration 78 took: 15.95 seconds\n",
            "--- Iteration 79/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0148\n",
            "  Iteration 79 took: 16.34 seconds\n",
            "--- Iteration 80/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0149\n",
            "  Iteration 80 took: 16.41 seconds\n",
            "--- Iteration 81/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0149\n",
            "  Iteration 81 took: 16.37 seconds\n",
            "--- Iteration 82/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0150\n",
            "  Iteration 82 took: 16.07 seconds\n",
            "--- Iteration 83/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0150\n",
            "  Iteration 83 took: 15.44 seconds\n",
            "--- Iteration 84/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0151\n",
            "  Iteration 84 took: 15.55 seconds\n",
            "--- Iteration 85/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0151\n",
            "  Iteration 85 took: 15.44 seconds\n",
            "--- Iteration 86/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0152\n",
            "  Iteration 86 took: 15.07 seconds\n",
            "--- Iteration 87/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0152\n",
            "  Iteration 87 took: 15.55 seconds\n",
            "--- Iteration 88/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0153\n",
            "  Iteration 88 took: 15.47 seconds\n",
            "--- Iteration 89/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0153\n",
            "  Iteration 89 took: 16.19 seconds\n",
            "--- Iteration 90/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0154\n",
            "  Iteration 90 took: 16.16 seconds\n",
            "--- Iteration 91/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0154\n",
            "  Iteration 91 took: 15.87 seconds\n",
            "--- Iteration 92/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0155\n",
            "  Iteration 92 took: 16.00 seconds\n",
            "--- Iteration 93/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0155\n",
            "  Iteration 93 took: 16.04 seconds\n",
            "--- Iteration 94/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0156\n",
            "  Iteration 94 took: 15.69 seconds\n",
            "--- Iteration 95/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0156\n",
            "  Iteration 95 took: 16.17 seconds\n",
            "--- Iteration 96/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0157\n",
            "  Iteration 96 took: 15.22 seconds\n",
            "--- Iteration 97/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0157\n",
            "  Iteration 97 took: 15.19 seconds\n",
            "--- Iteration 98/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0158\n",
            "  Iteration 98 took: 15.49 seconds\n",
            "--- Iteration 99/100 ---\n",
            "  Swap accepted! Node 3976 moved from 7 to 13, and Node 1981 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000050. New score: 0.0158\n",
            "  Iteration 99 took: 15.91 seconds\n",
            "--- Iteration 100/100 ---\n",
            "  Swap accepted! Node 1981 moved from 7 to 13, and Node 3976 moved from 13 to 7.\n",
            "  Modularity increased by: 0.000052. New score: 0.0159\n",
            "  Iteration 100 took: 15.95 seconds\n",
            "\n",
            "--- Final Results ---\n",
            "Optimal Modularity Score (after local swaps): 0.0159\n",
            "Final Partition Size: 22\n",
            "Final Modularity Score: 0.015875545022252805\n",
            "Optimization Time: 1567.1480712890625\n"
          ]
        }
      ],
      "source": [
        "N_iter = 100\n",
        "best_partition, final_score, duration = optimized_sequential_swap(G, Geo, partGeo, N_iterations=N_iter)\n",
        "print(f\"Final Partition Size: {len(best_partition)}\")\n",
        "print(f\"Final Modularity Score: {final_score}\")\n",
        "print(f\"Optimization Time: {duration}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "7d6bf27b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully saved the best partition and score to best_partition_100_0.0159.pkl\n"
          ]
        }
      ],
      "source": [
        "# Saving best partition\n",
        "\n",
        "# Assuming 'best_partition' is the result from your optimization function\n",
        "# and 'final_score' is the final modularity score.\n",
        "N_iter = 100\n",
        "PARTITION_FILENAME = f'best_partition_{N_iter}_{final_score:.4f}.pkl'\n",
        "\n",
        "# Save the partition and the final score to a file\n",
        "with open(PARTITION_FILENAME, 'wb') as f:\n",
        "    # We save a tuple containing both the partition and the score\n",
        "    pickle.dump((best_partition, final_score), f)\n",
        "\n",
        "print(f\"Successfully saved the best partition and score to {PARTITION_FILENAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6c3484c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading saved partition\n",
        "\n",
        "PARTITION_FILENAME = 'best_nc_partition.pkl'\n",
        "\n",
        "try:\n",
        "    with open(PARTITION_FILENAME, 'rb') as f:\n",
        "        # Load the tuple back\n",
        "        loaded_partition, loaded_score = pickle.load(f)\n",
        "    \n",
        "    print(f\"Successfully loaded partition from {PARTITION_FILENAME}\")\n",
        "    print(f\"Loaded Partition has {len(loaded_partition)} districts.\")\n",
        "    print(f\"Loaded Modularity Score: {loaded_score:.4f}\")\n",
        "    \n",
        "    # You can now use loaded_partition for plotting or further analysis\n",
        "    # e.g., loaded_partition is your new 'best_partition'\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File {PARTITION_FILENAME} not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4c73a73-e2e2-4b0b-8661-05d9c2976fcf",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "from plot_geo import plot_partition_geographically\n",
        "\n",
        "SHAPEFILE = \"tl_2020_37_tract/tl_2020_37_tract.shp\" \n",
        "plot_partition_geographically(data, best_partition, SHAPEFILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7de36f0",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
