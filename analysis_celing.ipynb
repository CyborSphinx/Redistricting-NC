{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f646945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA METHOD WITH REPORTING\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import networkx as nx\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def reduce_pca_by_variance(data: np.ndarray, feature_names: list, variance_threshold: float):\n",
    "    \"\"\"\n",
    "    Performs PCA on n-dimensional data, automatically selecting the minimum\n",
    "    number of components to explain at least the `variance_threshold`.\n",
    "    \n",
    "    This modified version also prints the results of the reduction and\n",
    "    the top 5 feature contributors for each component.\n",
    "\n",
    "    Args:\n",
    "        data: A (n_samples, n_features) NumPy array.\n",
    "        feature_names: A list of strings corresponding to the feature columns\n",
    "                       in `data`. (e.g., list(df.columns))\n",
    "        variance_threshold: The target amount of variance to explain\n",
    "                            (e.g., 0.95 for 95%).\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - data_transformed (np.ndarray): The data projected onto the\n",
    "                                         new component space.\n",
    "        - fitted_pca (PCA): The fitted PCA object, which you can use\n",
    "                            to inspect the number of components, etc.\n",
    "        - explained_variance_list (list): A list of the variance explained\n",
    "                                          by each component (e.g., [0.5, 0.3]).\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(feature_names) != data.shape[1]:\n",
    "        raise ValueError(f\"Number of feature_names ({len(feature_names)}) does not \"\n",
    "                         f\"match number of data columns ({data.shape[1]}).\")\n",
    "\n",
    "    # 1. Create a PCA object with the variance threshold.\n",
    "    # By setting n_components to a float, PCA automatically finds\n",
    "    # the components needed to explain that much variance.\n",
    "    pca = PCA(n_components=variance_threshold)\n",
    "    \n",
    "    # 2. Create a pipeline to first scale the data, then run PCA.\n",
    "    # Scaling is crucial for PCA to work correctly.\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler(with_std=False)),\n",
    "        ('pca', pca)\n",
    "    ])\n",
    "    \n",
    "    # 3. Fit the pipeline to the data and transform it\n",
    "    data_transformed = pipeline.fit_transform(data)\n",
    "    \n",
    "    # --- Report print statements ---\n",
    "    \n",
    "    # Get the original and new dimensions\n",
    "    original_dimensions = data.shape[1]\n",
    "    # We access the fitted pca object from step 2\n",
    "    new_dimensions = pca.n_components_ \n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(\"PCA Dimensionality Reduction Report\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Original dimensions:   {original_dimensions}\")\n",
    "    print(f\"New dimensions:        {new_dimensions}\")\n",
    "    print(f\"Dimensions reduced by: {original_dimensions - new_dimensions}\")\n",
    "    print(\"\\nVariance explained by each remaining component:\")\n",
    "    \n",
    "    # pca.explained_variance_ratio_ is an array like [0.5, 0.3, 0.1]\n",
    "    for i, variance in enumerate(pca.explained_variance_ratio_):\n",
    "        print(f\"  Principal Component {i+1}: {variance * 100:.2f}%\")\n",
    "        \n",
    "    # Print total variance explained\n",
    "    total_variance = np.sum(pca.explained_variance_ratio_)\n",
    "    print(f\"\\nTotal variance explained: {total_variance * 100:.2f}%\")\n",
    "    print(f\"(Target threshold was {variance_threshold * 100:.0f}%)\")\n",
    "    \n",
    "    # --- New: Top 5 Contributors per Component Report ---\n",
    "    print(\"\\nTop 5 Contributors per Component:\")\n",
    "    \n",
    "    # pca.components_ has shape (n_components, n_features)\n",
    "    for i, component in enumerate(pca.components_):\n",
    "        print(f\"  --- Principal Component {i+1} ---\")\n",
    "        \n",
    "        # Get indices of the top 5 absolute loadings\n",
    "        # np.argsort returns indices of smallest to largest\n",
    "        # We take the last 5, and then reverse them [::-1]\n",
    "        top_5_indices = np.argsort(np.abs(component))[-5:][::-1]\n",
    "        \n",
    "        # Print the feature name and its loading (weight)\n",
    "        for j, feature_index in enumerate(top_5_indices):\n",
    "            feature_name = feature_names[feature_index]\n",
    "            loading = component[feature_index]\n",
    "            print(f\"    {j+1}. {feature_name}: {loading:.4f}\")\n",
    "            \n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # --- End of report ---\n",
    "    \n",
    "    # Get the list of explained variances\n",
    "    explained_variance_list = pca.explained_variance_ratio_.tolist()\n",
    "    \n",
    "    # Return the new data, the fitted PCA object, and the list of variances\n",
    "    return data_transformed, pca, explained_variance_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b70efe",
   "metadata": {},
   "source": [
    "# Processing and Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d69100f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get data to be a numpy array probably\n",
    "df=pd.read_csv('consolidated_with_cd_118_116.csv')\n",
    "data=np.array(df)\n",
    "income_index = df.columns.get_loc('per_capita_income')\n",
    "# Transform the income dimension by logarithmic scale\n",
    "data[:, income_index] = np.log(data[:, income_index].astype(np.float64))\n",
    "# Normalize data by dividing by standard deviation by dimension\n",
    "for i in range(2,data.shape[1]-2):\n",
    "    data[:, i] = data[:, i] / np.std(data[:, i])\n",
    "weights = np.array([\n",
    "    0.1225,     # 1:  per_capita_income (Economic Security - Income)\n",
    "    0.2/6,      # 2:  white (Cultural - Race)\n",
    "    0.2/6,      # 3:  black (Cultural - Race)\n",
    "    0.2/6,      # 4:  asian (Cultural - Race)\n",
    "    0.2/6,      # 5:  native (Cultural - Race)\n",
    "    0.2/6,      # 6:  pacific islander (Cultural - Race)\n",
    "    0.2/6,      # 7:  other (Cultural - Race)\n",
    "    0.05,       # 8:  Under High School (not weighted)\n",
    "    0.05,       # 9:  High School (No College Degree) (not weighted)\n",
    "    0.05,       # 10: College or More (Education)\n",
    "    0.0,        # 11: Agriculture (not weighted)\n",
    "    0.0,        # 12: Construction_and_manufacturing (not weighted)\n",
    "    0.0,        # 13: trade (not weighted)\n",
    "    0.0,        # 14: Transportation and warehousing (not weighted)\n",
    "    0.0,        # 15: nerds (not weighted)\n",
    "    0.0,      # 16: Educational services, and health care (Economic Security - Healthcare)\n",
    "    0.0,        # 17: finance_inurance_and_realty (not weighted)\n",
    "    0.0,        # 18: other_services (not weighted)\n",
    "    0.07,       # 19: in_labor_force (Economic Security - Employment)\n",
    "    0.0,        # 20: out_labor_force (not weighted)\n",
    "    0.15,       # 21: avg_commute_time (Location Affordability - Transportation)\n",
    "    0.15,       # 22: avg_housing_cost_burden (Location Affordability - Housing)\n",
    "    0.0525,     # 23: avg_poverty_ratio (Economic Security - Poverty)\n",
    "])\n",
    "for i in range(data.shape[1]-4):\n",
    "    data[:, i+2] = data[:, i+2] * weights[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "790ecf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "PCA Dimensionality Reduction Report\n",
      "------------------------------\n",
      "Original dimensions:   23\n",
      "New dimensions:        4\n",
      "Dimensions reduced by: 19\n",
      "\n",
      "Variance explained by each remaining component:\n",
      "  Principal Component 1: 31.37%\n",
      "  Principal Component 2: 26.43%\n",
      "  Principal Component 3: 18.92%\n",
      "  Principal Component 4: 6.65%\n",
      "\n",
      "Total variance explained: 83.36%\n",
      "(Target threshold was 80%)\n",
      "\n",
      "Top 5 Contributors per Component:\n",
      "  --- Principal Component 1 ---\n",
      "    1. avg_housing_cost_burden: 0.7903\n",
      "    2. avg_commute_time: -0.5353\n",
      "    3. per_capita_income: -0.2125\n",
      "    4. avg_poverty_ratio: -0.1654\n",
      "    5. Under High School: 0.0741\n",
      "  --- Principal Component 2 ---\n",
      "    1. avg_commute_time: 0.8283\n",
      "    2. avg_housing_cost_burden: 0.4448\n",
      "    3. per_capita_income: -0.2472\n",
      "    4. College or More: -0.1207\n",
      "    5. avg_poverty_ratio: -0.1123\n",
      "  --- Principal Component 3 ---\n",
      "    1. per_capita_income: 0.8816\n",
      "    2. avg_housing_cost_burden: 0.3618\n",
      "    3. College or More: 0.1741\n",
      "    4. High School (No College Degree): -0.1685\n",
      "    5. avg_commute_time: 0.1294\n",
      "  --- Principal Component 4 ---\n",
      "    1. in_labor_force: 0.7985\n",
      "    2. Under High School: -0.3035\n",
      "    3. College or More: 0.3019\n",
      "    4. avg_poverty_ratio: 0.2713\n",
      "    5. per_capita_income: -0.2139\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Perform PCA, and project onto the top N dimensions so that they explain 70% of the variance\n",
    "new_data,pca,var_explained = reduce_pca_by_variance(data[:,2:-2], list(df.columns)[2:-2], variance_threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3679f479",
   "metadata": {},
   "source": [
    "# Network initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a811b662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dimension_layered_knn(data, dimension_weights, k=10):\n",
    "    \"\"\"\n",
    "    Create multi-layer network where each dimension has its own KNN graph.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for i in range(data.shape[0]):\n",
    "        G.add_node(i)\n",
    "    \n",
    "    # For each dimension, create KNN graph\n",
    "    for dim in range(data.shape[1]):\n",
    "        dim_weight = dimension_weights[dim]\n",
    "        \n",
    "        if dim_weight == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get this dimension's values (1D)\n",
    "        dim_data = data[:, dim].reshape(-1, 1)\n",
    "        \n",
    "        # Build KNN graph for THIS dimension only\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        nbrs = NearestNeighbors(n_neighbors=k)\n",
    "        nbrs.fit(dim_data)\n",
    "        distances, indices = nbrs.kneighbors(dim_data)\n",
    "        \n",
    "        # Add edges weighted by dimension importance\n",
    "        for i in range(len(data)):\n",
    "            for j, neighbor in enumerate(indices[i]):\n",
    "                if i != neighbor:\n",
    "                    if G.has_edge(i, neighbor):\n",
    "                        G[i][neighbor]['weight'] += dim_weight  # Accumulate\n",
    "                    else:\n",
    "                        G.add_edge(i, neighbor, weight=dim_weight)\n",
    "    \n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7919abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data topology graph\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def build_topology_graph(data, weights, rmax_scale, rdisc_scale, k, scaling_method='std'):\n",
    "    \"\"\"\n",
    "    Builds a dimension-wise gap graph where connection radii are scaled \n",
    "    by the statistical spread (Variance or StdDev) of each dimension.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : np.ndarray (N, D)\n",
    "    weights : list or np.array (D,)\n",
    "        Importance weight for each dimension.\n",
    "    rmax_scale : float\n",
    "        The base scaling factor for the maximum radius. \n",
    "        Actual R_max[d] = rmax_scale * Variance[d]\n",
    "    rdisc_scale : float\n",
    "        The base scaling factor for the discounting radius.\n",
    "        Actual R_disc[d] = rdisc_scale * Variance[d]\n",
    "    k : int\n",
    "        Max neighbors per side (1D).\n",
    "    scaling_method : str\n",
    "        'variance' (default) -> Scale by sigma^2.\n",
    "        'std' -> Scale by sigma (standard deviation).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    nx.MultiGraph\n",
    "    \"\"\"\n",
    "    N, D = data.shape\n",
    "    G = nx.MultiGraph()\n",
    "    G.add_nodes_from(range(N))\n",
    "    \n",
    "    # 1. Calculate Statistics for Scaling\n",
    "    if scaling_method == 'variance':\n",
    "        # Add a tiny epsilon to prevent 0-radius in constant dimensions\n",
    "        metric = np.var(data, axis=0) + 1e-9\n",
    "        #print(\"Scaling radii by Dimension Variance.\")\n",
    "    elif scaling_method == 'std':\n",
    "        metric = np.std(data, axis=0) + 1e-9\n",
    "        #print(\"Scaling radii by Dimension Standard Deviation.\")\n",
    "    else:\n",
    "        raise ValueError(\"scaling_method must be 'variance' or 'std'\")\n",
    "\n",
    "    # Calculate the specific thresholds for each dimension\n",
    "    # R_max[d] = Scale * Metric[d]\n",
    "    dim_rmaxs = rmax_scale * metric\n",
    "    dim_rdiscs = rdisc_scale * metric\n",
    "\n",
    "    #print(f\"Processing {D} dimensions for {N} nodes...\")\n",
    "    \n",
    "    for d in range(D):\n",
    "        # Retrieve specific thresholds for this dimension\n",
    "        dim_weight = weights[d]\n",
    "        current_rmax = dim_rmaxs[d]\n",
    "        current_rdisc = dim_rdiscs[d]\n",
    "        \n",
    "        # Sort data for sliding window\n",
    "        sorted_indices = np.argsort(data[:, d])\n",
    "        sorted_vals = data[sorted_indices, d]\n",
    "        \n",
    "        # Sliding Window (Vectorized)\n",
    "        for shift in range(1, k + 1):\n",
    "            u_indices = sorted_indices[:-shift]\n",
    "            v_indices = sorted_indices[shift:]\n",
    "            \n",
    "            # Calculate 1D distances\n",
    "            dists = sorted_vals[shift:] - sorted_vals[:-shift]\n",
    "            \n",
    "            # --- GAP DETECTION ---\n",
    "            # Use the variance-scaled R_max for this specific dimension\n",
    "            valid_mask = dists <= current_rmax\n",
    "            \n",
    "            if not np.any(valid_mask):\n",
    "                continue\n",
    "\n",
    "            # Filter\n",
    "            valid_u = u_indices[valid_mask]\n",
    "            valid_v = v_indices[valid_mask]\n",
    "            valid_dists = dists[valid_mask]\n",
    "            \n",
    "            # --- DISCOUNTING ---\n",
    "            # Base weight\n",
    "            edge_weights = np.full(valid_dists.shape, dim_weight, dtype=float)\n",
    "            \n",
    "            # Check against variance-scaled Discount Radius\n",
    "            discount_mask = valid_dists > current_rdisc\n",
    "            \n",
    "            # Apply decay: w = w * (r_disc / dist)\n",
    "            safe_dists = valid_dists.copy()\n",
    "            safe_dists[safe_dists == 0] = 1e-9 \n",
    "            \n",
    "            edge_weights[discount_mask] *= (current_rdisc / safe_dists[discount_mask])\n",
    "            \n",
    "            # Add to graph\n",
    "            edges_to_add = zip(\n",
    "                valid_u, \n",
    "                valid_v, \n",
    "                [{'weight': w, 'dimension': d} for w in edge_weights]\n",
    "            )\n",
    "            G.add_edges_from(edges_to_add)\n",
    "\n",
    "    #print(f\"Done. Edges: {G.number_of_edges()}\")\n",
    "    return G\n",
    "\n",
    "def flatten_graph_for_community_detection(G_multi):\n",
    "    \"\"\"\n",
    "    Converts the MultiGraph into a simple Weighted Graph by summing weights.\n",
    "    Required for Louvain/Leiden algorithms.\n",
    "    \"\"\"\n",
    "    G_simple = nx.Graph()\n",
    "    G_simple.add_nodes_from(G_multi.nodes)\n",
    "    for u, v, data in G_multi.edges(data=True):\n",
    "        w = data['weight']\n",
    "        if G_simple.has_edge(u, v):\n",
    "            G_simple[u][v]['weight'] += w\n",
    "        else:\n",
    "            G_simple.add_edge(u, v, weight=w)\n",
    "            \n",
    "    return G_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cd4ad9",
   "metadata": {},
   "source": [
    "## Geographic weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e5377c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from scipy.sparse import load_npz\n",
    "def add_adjacency_edges_with_weight(G, weight):\n",
    "    \"\"\"\n",
    "    Adds edges to graph G based on a precomputed adjacency matrix,\n",
    "    assigning a uniform weight to each added edge.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    G : nx.Graph\n",
    "        The input graph to which edges will be added.\n",
    "    weight : float\n",
    "        The weight to assign to each added edge.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None (modifies G in place)\n",
    "    \"\"\"\n",
    "    # Create graph\n",
    "    Geo = G.copy()\n",
    "\n",
    "    # Load adjacency matrix\n",
    "    adj_matrix = load_npz('adjacency_queen_matrix.npz')\n",
    "\n",
    "    # Load mappings\n",
    "    with open('adjacency_queen_mappings.pkl', 'rb') as f:\n",
    "        mappings = pickle.load(f)\n",
    "    index_to_geoid = mappings['index_to_geoid']\n",
    "\n",
    "    # Get edges from sparse matrix\n",
    "    rows, cols = adj_matrix.nonzero()\n",
    "\n",
    "    # Add edges with your weight\n",
    "    weight = 2  # Change this to your desired weight\n",
    "\n",
    "    for i, j in zip(rows, cols):\n",
    "        if i < j:  # Only add each edge once (undirected)\n",
    "            geoid1 = index_to_geoid[i]\n",
    "            geoid2 = index_to_geoid[j]\n",
    "            Geo.add_edge(i, j, weight=weight)\n",
    "            #Gring_geo.add_edge(i, j, weight=weight)\n",
    "    return Geo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "07a3491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "from typing import List\n",
    "def is_contiguous(G: nx.Graph, partition: List) -> bool:\n",
    "    for part in partition:\n",
    "        if not nx.is_connected(G.subgraph(part)):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def generate_geopure(): # give it adj_matrix from the geoweight section\n",
    "    rows, cols = load_npz('adjacency_queen_matrix.npz').nonzero()\n",
    "    pure=nx.Graph()\n",
    "    for i, j in zip(rows, cols):\n",
    "        if i < j:  # Only add each edge once (undirected)\n",
    "            pure.add_edge(int(i), int(j))\n",
    "    res=0.3\n",
    "    n=13\n",
    "    cont=False\n",
    "    while n!=14 or not cont:\n",
    "        res*=(14/n)**0.5\n",
    "        pure_part=nx.algorithms.community.louvain_communities(pure, weight='weight', resolution=res)\n",
    "        cont=is_contiguous(pure, pure_part)\n",
    "        n=len(pure_part)\n",
    "        # print(\"iterating with resolution:\", res, \" got \", n, \" communities\",\"contiguous:\", cont)\n",
    "    return pure,pure_part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ca54260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterating with resolution: 0.6226494259953249  got  23  communities contiguous: True\n",
      "iterating with resolution: 0.48578454285164174  got  20  communities contiguous: True\n",
      "iterating with resolution: 0.40643650851209834  got  17  communities contiguous: True\n",
      "iterating with resolution: 0.36883512148711634  got  17  communities contiguous: True\n",
      "iterating with resolution: 0.3347124187746692  got  16  communities contiguous: True\n",
      "iterating with resolution: 0.31309479853830346  got  16  communities contiguous: True\n",
      "iterating with resolution: 0.29287336642783557  got  16  communities contiguous: True\n",
      "iterating with resolution: 0.27395794872101553  got  14  communities contiguous: True\n",
      "113.43354163274167\n",
      "iterating with resolution: 0.6226494259953249  got  24  communities contiguous: True\n",
      "iterating with resolution: 0.4755563543407302  got  19  communities contiguous: True\n",
      "iterating with resolution: 0.4082152325836951  got  19  communities contiguous: True\n",
      "iterating with resolution: 0.35040994530369585  got  16  communities contiguous: True\n",
      "iterating with resolution: 0.32777849006115656  got  16  communities contiguous: True\n",
      "iterating with resolution: 0.3066087021407338  got  14  communities contiguous: True\n",
      "145.91444585853756\n",
      "iterating with resolution: 0.6226494259953249  got  25  communities contiguous: True\n",
      "iterating with resolution: 0.46594816482919243  got  18  communities contiguous: True\n",
      "iterating with resolution: 0.4109276559949919  got  17  communities contiguous: True\n",
      "iterating with resolution: 0.3729107714171233  got  18  communities contiguous: True\n",
      "iterating with resolution: 0.3288763874623206  got  16  communities contiguous: True\n",
      "iterating with resolution: 0.3076356911209802  got  15  communities contiguous: True\n",
      "iterating with resolution: 0.2972043133738993  got  15  communities contiguous: True\n",
      "iterating with resolution: 0.2871266450462482  got  15  communities contiguous: True\n",
      "iterating with resolution: 0.27739069248230597  got  15  communities contiguous: True\n",
      "iterating with resolution: 0.2679848687098316  got  13  communities contiguous: True\n",
      "iterating with resolution: 0.27810104112934864  got  13  communities contiguous: True\n",
      "iterating with resolution: 0.28859908937981865  got  15  communities contiguous: True\n",
      "iterating with resolution: 0.27881320885401006  got  16  communities contiguous: True\n",
      "iterating with resolution: 0.2608058756096881  got  15  communities contiguous: True\n",
      "iterating with resolution: 0.2519624134053206  got  14  communities contiguous: True\n",
      "121.20359410714599\n",
      "iterating with resolution: 0.6226494259953249  got  22  communities contiguous: True\n",
      "iterating with resolution: 0.4967024126308293  got  21  communities contiguous: True\n",
      "iterating with resolution: 0.4055558216516247  got  18  communities contiguous: True\n",
      "iterating with resolution: 0.3576666156148878  got  16  communities contiguous: True\n",
      "iterating with resolution: 0.33456648357947016  got  16  communities contiguous: True\n",
      "iterating with resolution: 0.3129582886630268  got  15  communities contiguous: True\n",
      "iterating with resolution: 0.3023464311239085  got  14  communities contiguous: True\n",
      "180.5436941856571\n",
      "iterating with resolution: 0.6226494259953249  got  23  communities contiguous: True\n",
      "iterating with resolution: 0.48578454285164174  got  19  communities contiguous: True\n",
      "iterating with resolution: 0.41699505923048635  got  17  communities contiguous: True\n",
      "iterating with resolution: 0.3784168501344792  got  17  communities contiguous: True\n",
      "iterating with resolution: 0.34340769583687103  got  16  communities contiguous: True\n",
      "iterating with resolution: 0.3212284854507618  got  16  communities contiguous: True\n",
      "iterating with resolution: 0.3004817338572621  got  14  communities contiguous: True\n",
      "171.79064441989874\n",
      "iterating with resolution: 0.6226494259953249  got  23  communities contiguous: True\n",
      "iterating with resolution: 0.48578454285164174  got  19  communities contiguous: True\n",
      "iterating with resolution: 0.41699505923048635  got  17  communities contiguous: True\n",
      "iterating with resolution: 0.3784168501344792  got  17  communities contiguous: True\n",
      "iterating with resolution: 0.34340769583687103  got  15  communities contiguous: True\n",
      "iterating with resolution: 0.33176335319419525  got  14  communities contiguous: True\n",
      "162.360788093074\n",
      "iterating with resolution: 0.6226494259953249  got  26  communities contiguous: True\n",
      "iterating with resolution: 0.4568997663051537  got  19  communities contiguous: True\n",
      "iterating with resolution: 0.39220050929244804  got  18  communities contiguous: True\n",
      "iterating with resolution: 0.34588833722023155  got  15  communities contiguous: True\n",
      "iterating with resolution: 0.3341598804514263  got  16  communities contiguous: True\n",
      "iterating with resolution: 0.3125779462636441  got  17  communities contiguous: True\n",
      "iterating with resolution: 0.2836598641357354  got  13  communities contiguous: True\n",
      "iterating with resolution: 0.29436775263671255  got  14  communities contiguous: True\n",
      "157.89787774355415\n",
      "iterating with resolution: 0.6226494259953249  got  22  communities contiguous: True\n",
      "iterating with resolution: 0.4967024126308293  got  21  communities contiguous: True\n",
      "iterating with resolution: 0.4055558216516247  got  19  communities contiguous: True\n",
      "iterating with resolution: 0.3481271200564636  got  16  communities contiguous: True\n",
      "iterating with resolution: 0.32564310257390144  got  15  communities contiguous: True\n",
      "iterating with resolution: 0.3146011256130945  got  14  communities contiguous: True\n",
      "156.75694496869468\n",
      "iterating with resolution: 0.6226494259953249  got  23  communities contiguous: True\n",
      "iterating with resolution: 0.48578454285164174  got  21  communities contiguous: True\n",
      "iterating with resolution: 0.3966414183059039  got  16  communities contiguous: True\n",
      "iterating with resolution: 0.3710240731761945  got  17  communities contiguous: True\n",
      "iterating with resolution: 0.33669886006441996  got  15  communities contiguous: True\n",
      "iterating with resolution: 0.3252820020804018  got  16  communities contiguous: True\n",
      "iterating with resolution: 0.304273451467188  got  13  communities contiguous: True\n",
      "iterating with resolution: 0.315759483169435  got  15  communities contiguous: True\n",
      "iterating with resolution: 0.30505264211935634  got  15  communities contiguous: True\n",
      "iterating with resolution: 0.2947088509581393  got  13  communities contiguous: True\n",
      "iterating with resolution: 0.3058338281413787  got  15  communities contiguous: True\n",
      "iterating with resolution: 0.2954635483550715  got  16  communities contiguous: True\n",
      "iterating with resolution: 0.2763808420562982  got  12  communities contiguous: True\n",
      "iterating with resolution: 0.29852542856241443  got  15  communities contiguous: True\n",
      "iterating with resolution: 0.2884029635743739  got  15  communities contiguous: True\n",
      "iterating with resolution: 0.2786237333249201  got  14  communities contiguous: True\n",
      "117.41938180934949\n",
      "iterating with resolution: 0.6226494259953249  got  23  communities contiguous: True\n",
      "iterating with resolution: 0.48578454285164174  got  19  communities contiguous: True\n",
      "iterating with resolution: 0.41699505923048635  got  16  communities contiguous: True\n",
      "iterating with resolution: 0.3900631609044966  got  17  communities contiguous: True\n",
      "iterating with resolution: 0.3539765506463504  got  17  communities contiguous: True\n",
      "iterating with resolution: 0.3212284854507619  got  16  communities contiguous: True\n",
      "iterating with resolution: 0.3004817338572622  got  14  communities contiguous: True\n",
      "187.650578991695\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for i in range(10):\n",
    "#     pure,part=generate_geopure()\n",
    "#     print(np.std([len(x) for x in part]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd9f06b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Experiment 1: Varying K (Standard KNN) ---\n",
      "K     | Edges      | Q (Real)   | Q (Rand)   | Diff      \n",
      "-------------------------------------------------------\n",
      "2     | 5872       | 0.6108     | 0.5859     | 0.0249\n",
      "3     | 11745      | 0.4645     | 0.3723     | 0.0922\n",
      "4     | 17618      | 0.4149     | 0.2835     | 0.1314\n",
      "5     | 23491      | 0.3947     | 0.2353     | 0.1593\n",
      "6     | 29364      | 0.3824     | 0.1992     | 0.1833\n",
      "7     | 35237      | 0.3763     | 0.1791     | 0.1971\n",
      "8     | 41110      | 0.3730     | 0.1647     | 0.2082\n",
      "9     | 46983      | 0.3709     | 0.1705     | 0.2005\n",
      "10    | 52856      | 0.3695     | 0.1607     | 0.2088\n",
      "11    | 58729      | 0.3686     | 0.1520     | 0.2166\n",
      "12    | 64601      | 0.3675     | 0.1440     | 0.2235\n",
      "13    | 70474      | 0.3668     | 0.1380     | 0.2288\n",
      "14    | 76347      | 0.3657     | 0.1299     | 0.2358\n",
      "15    | 82220      | 0.3650     | 0.1246     | 0.2404\n",
      "16    | 88093      | 0.3645     | 0.1231     | 0.2414\n",
      "17    | 93966      | 0.3642     | 0.1167     | 0.2475\n",
      "18    | 99839      | 0.3632     | 0.1116     | 0.2516\n",
      "19    | 105712     | 0.3626     | 0.1139     | 0.2488\n",
      "20    | 111585     | 0.3620     | 0.1097     | 0.2523\n",
      "\n",
      "\n",
      "--- Experiment 2: Varying R_max (Topology Graph) ---\n",
      "R_max      | Edges      | Q (Real)   | Q (Rand)   | Diff      \n",
      "------------------------------------------------------------\n",
      "0.00005    | 370        | 0.9978     | 0.9968     | 0.0010\n",
      "0.00010    | 729        | 0.9814     | 0.9597     | 0.0217\n",
      "0.00020    | 1448       | 0.8779     | 0.7907     | 0.0871\n",
      "0.00040    | 2885       | 0.7043     | 0.5544     | 0.1499\n",
      "0.00080    | 5744       | 0.5425     | 0.3476     | 0.1949\n",
      "0.00160    | 11476      | 0.4291     | 0.2135     | 0.2156\n",
      "0.00320    | 22279      | 0.3818     | 0.1381     | 0.2437\n",
      "0.00640    | 36572      | 0.3656     | 0.1248     | 0.2409\n",
      "0.01280    | 48820      | 0.3598     | 0.1121     | 0.2477\n",
      "0.02560    | 54823      | 0.3564     | 0.1112     | 0.2452\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# --- 1. Helper Functions (Re-defined for completeness) ---\n",
    "\n",
    "def randomize_graph(G):\n",
    "    \"\"\"\n",
    "    Creates a random graph with the same Nodes (N), Edges (M), and \n",
    "    Weight Distribution as G, but random topology.\n",
    "    \"\"\"\n",
    "    N = G.number_of_nodes()\n",
    "    M = G.number_of_edges()\n",
    "    \n",
    "    # 1. Extract weights from original graph\n",
    "    weights = [d['weight'] for u, v, d in G.edges(data=True)]\n",
    "    \n",
    "    # 2. Create random graph with same density (Erdos-Renyi variant)\n",
    "    G_rand = nx.gnm_random_graph(N, M, seed=42)\n",
    "    \n",
    "    # 3. Assign shuffled weights to the new random edges\n",
    "    random.shuffle(weights)\n",
    "    for i, (u, v) in enumerate(G_rand.edges()):\n",
    "        G_rand[u][v]['weight'] = weights[i]\n",
    "        \n",
    "    return G_rand\n",
    "\n",
    "def get_modularity(G):\n",
    "    \"\"\"Runs Louvain and returns modularity score.\"\"\"\n",
    "    if G.number_of_edges() == 0: return 0\n",
    "    try:\n",
    "        part = nx.algorithms.community.louvain_communities(G, weight='weight', resolution=1)\n",
    "        return nx.algorithms.community.modularity(G, part, weight='weight')\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# --- 2. Execution Loops ---\n",
    "\n",
    "# Assumes 'new_data' and 'var_explained' are already defined in your notebook\n",
    "print(\"--- Experiment 1: Varying K (Standard KNN) ---\")\n",
    "print(f\"{'K':<5} | {'Edges':<10} | {'Q (Real)':<10} | {'Q (Rand)':<10} | {'Diff':<10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for k in range(2, 21):\n",
    "    # 1. Build Real Graph\n",
    "    Gk = create_dimension_layered_knn(new_data, dimension_weights=var_explained, k=k)\n",
    "    q_real = get_modularity(Gk)\n",
    "    \n",
    "    # 2. Build Random Graph & Measure\n",
    "    Gk_rand = randomize_graph(Gk)\n",
    "    q_rand = get_modularity(Gk_rand)\n",
    "    \n",
    "    total_weight = Gk.size(weight='weight')\n",
    "    \n",
    "    print(f\"{k:<5} | {int(total_weight):<10} | {q_real:.4f}     | {q_rand:.4f}     | {q_real - q_rand:.4f}\")\n",
    "\n",
    "print(\"\\n\\n--- Experiment 2: Varying R_max (Topology Graph) ---\")\n",
    "print(f\"{'R_max':<10} | {'Edges':<10} | {'Q (Real)':<10} | {'Q (Rand)':<10} | {'Diff':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i in range(1, 11):\n",
    "    # 1. Build Real Graph\n",
    "    rmax = 0.0002 * 2**(i-3)\n",
    "    rdisc = rmax / 4\n",
    "    \n",
    "    Gmulti = build_topology_graph(new_data, var_explained, rmax, rdisc, k=10, scaling_method=\"std\")\n",
    "    Gtopo = flatten_graph_for_community_detection(Gmulti)\n",
    "    q_real = get_modularity(Gtopo)\n",
    "    \n",
    "    # 2. Build Random Graph & Measure\n",
    "    Gtopo_rand = randomize_graph(Gtopo)\n",
    "    q_rand = get_modularity(Gtopo_rand)\n",
    "    \n",
    "    total_weight = Gtopo.size(weight='weight')\n",
    "    \n",
    "    print(f\"{rmax:<10.5f} | {int(total_weight):<10} | {q_real:.4f}     | {q_rand:.4f}     | {q_real - q_rand:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9c5d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Graph Gtopo\n",
    "Gk = create_dimension_layered_knn(new_data, dimension_weights=var_explained, k=10)\n",
    "r_max=0.001\n",
    "r_disc=r_max/4\n",
    "Gmulti=build_topology_graph(data=new_data, weights=var_explained, rmax_scale=r_max, rdisc_scale=r_disc, k=10,scaling_method=\"std\")\n",
    "Gtopo=flatten_graph_for_community_detection(Gmulti)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7881e01d",
   "metadata": {},
   "source": [
    "For topological graph, there could be some redundency with the parameters. Having the same radius parameters, while increasing k,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39d08f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterating with resolution: 0.6226494259953249  got  22  communities contiguous: True\n",
      "iterating with resolution: 0.4967024126308293  got  18  communities contiguous: True\n",
      "iterating with resolution: 0.43805035314232066  got  20  communities contiguous: True\n",
      "iterating with resolution: 0.36649922008331515  got  18  communities contiguous: True\n",
      "iterating with resolution: 0.3232219306798603  got  17  communities contiguous: True\n",
      "iterating with resolution: 0.29331912260056703  got  14  communities contiguous: True\n"
     ]
    }
   ],
   "source": [
    "GeoPure,pure_part=generate_geopure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3492d8",
   "metadata": {},
   "source": [
    "# Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3da1e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K means clustering function\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_kmeans_partition(data: np.ndarray, weights, n_clusters=14):\n",
    "    \"\"\"\n",
    "    Runs K-means clustering on the input data and returns the loss\n",
    "    (inertia) and a partition of the data indices by cluster.\n",
    "\n",
    "    Args:\n",
    "        data: A (n_samples, n_features) NumPy array.\n",
    "        n_clusters: The number of clusters (k).\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - loss (float): The inertia (Within-Cluster Sum of Squares).\n",
    "        - partitions (dict): A dictionary where keys are cluster IDs (0 to k-1)\n",
    "                             and values are lists of original data indices\n",
    "                             belonging to that cluster.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Initialize and fit the K-means model\n",
    "    # n_init=10 runs the algorithm 10 times and picks the best result\n",
    "    # random_state=42 ensures the result is reproducible\n",
    "    data=data.copy()\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    for i in range(len(weights)):\n",
    "        data[:, i] = data[:, i] * weights[i]\n",
    "    kmeans.fit(data)\n",
    "\n",
    "    # 2. Get the loss (inertia)\n",
    "    # .inertia_ is the WCSS (Within-Cluster Sum of Squares)\n",
    "    loss = kmeans.inertia_\n",
    "\n",
    "    # 3. Get the cluster assignment for each data point\n",
    "    # .labels_ is an array like [0, 1, 1, 0, 2, ...]\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # 4. Create the partition of indices\n",
    "    partitions = {i: [] for i in range(n_clusters)}\n",
    "    for index, cluster_id in enumerate(labels):\n",
    "        partitions[cluster_id].append(index)\n",
    "\n",
    "    return loss, partitions\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def calculate_weighted_kmeans_loss_for_partition(data: np.ndarray, partition: list, weights: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the **Weighted** K-means \"loss\" (Inertia, or Within-Cluster Sum of Squares)\n",
    "    for a given dataset, partition, and a set of feature weights.\n",
    "\n",
    "    Args:\n",
    "        data: A (n_samples, n_features) NumPy array containing the unweighted, original data.\n",
    "        partition: A list of iterables (e.g., lists, sets, or tuples), where each inner iterable \n",
    "                   contains the *indices* (row numbers) of the data points belonging to that cluster.\n",
    "        weights: A (n_features,) NumPy array containing the weight for each dimension.\n",
    "\n",
    "    Returns:\n",
    "        total_loss (float): The total Weighted K-means loss for this partition.\n",
    "    \"\"\"\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Safety Check: Ensure the number of weights matches the number of features\n",
    "    if len(weights)!= data.shape[1]:\n",
    "        raise ValueError(\"The number of weights must match the number of features (columns) in the data.\")\n",
    "\n",
    "    # Iterate over each cluster (which is an iterable of indices)\n",
    "    for indices in partition:\n",
    "        \n",
    "        # --- FIX: Convert the index iterable (set/list/tuple) to a NumPy array for slicing ---\n",
    "        # This resolves the IndexError by providing valid index types to NumPy\n",
    "        cluster_indices = np.array(list(indices), dtype=int)\n",
    "        \n",
    "        # 1. Get all data points belonging to this cluster\n",
    "        cluster_points = data[cluster_indices, :]\n",
    "        \n",
    "        # Handle empty clusters (their loss is 0)\n",
    "        # Check if the cluster contains any points\n",
    "        if cluster_points.shape == 0:\n",
    "            continue\n",
    "            \n",
    "        # 2. Calculate the \"true\" centroid (mean) for this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        \n",
    "        # 3. Calculate the sum of *weighted* squared distances from each point to the centroid\n",
    "        \n",
    "        # Calculate squared differences for each feature: (Point - Centroid)^2\n",
    "        squared_diffs = (cluster_points - centroid) ** 2\n",
    "        \n",
    "        # Apply the weights: (Point - Centroid)^2 * Weight_j\n",
    "        weighted_squared_diffs = squared_diffs * weights\n",
    "        \n",
    "        # Sum all weighted squared differences to get the total cluster loss\n",
    "        # Summing twice: once over the dimensions (axis=1) and once over the points (np.sum)\n",
    "        cluster_loss = np.sum(weighted_squared_diffs)\n",
    "        \n",
    "        # 4. Add this cluster's loss to the total\n",
    "        total_loss += cluster_loss\n",
    "        \n",
    "    return total_loss#/data.shape[0]\n",
    "# Note on Usage:\n",
    "# You should pass the *unweighted* data array into this function,\n",
    "# since the weighting array is now passed separately as the 'weights' argument.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "k = 14\n",
    "\n",
    "# 3. Run the function\n",
    "total_loss, index_partitions = get_kmeans_partition(new_data,weights=var_explained, n_clusters=k)\n",
    "\n",
    "index_partitions=[index_partitions[i] for i in range(k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b56db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null partition model\n",
    "import random\n",
    "# Random partition of 14 parts model with no guarantee of size. \n",
    "part_random=[[] for i in range(14)]\n",
    "n=len(new_data)\n",
    "m=int(n/14)\n",
    "for i in range(n):\n",
    "    part_random[ random.choice(range(14)) ].append(i)\n",
    "# Partition model with 14 equal sized parts\n",
    "labels=[[i]*m for i in range(14)]\n",
    "eq_labels=[]\n",
    "for l in labels:\n",
    "    eq_labels+=l\n",
    "labels=random.shuffle(labels)\n",
    "part_equal=[[] for i in range(14)]\n",
    "for i in range(n):\n",
    "    try:part_equal[ labels[i] ].append(i)\n",
    "    except: part_equal[ random.choice(range(14)) ].append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb04a153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Louvain communities Gk: 18\n",
      "Number of Louvain communities Gtopo: 11\n"
     ]
    }
   ],
   "source": [
    "partGk=nx.algorithms.community.louvain_communities(Gk,weight='weight',resolution=0.7)\n",
    "print(\"Number of Louvain communities Gk:\",len(partGk))\n",
    "partGtopo=nx.algorithms.community.louvain_communities(Gtopo,weight='weight',resolution=0.5)\n",
    "print(\"Number of Louvain communities Gtopo:\",len(partGtopo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3da57ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse recent districting\n",
    "part116=[[] for i in range(13)]\n",
    "part118=[[] for i in range(14)]\n",
    "for i in range(data.shape[0]):\n",
    "    cd116=int(data[i,-1])-1\n",
    "    cd118=int(data[i,-2])-1\n",
    "    part116[cd116].append(i)\n",
    "    part118[cd118].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64e5c10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 10Graph Comparisons\n",
      "Louvain modularity Gtopo: 0.4475305196073563\n",
      "Louvain modularity GeoPure partition on Gtopo: 0.003154207294161239\n",
      "\n",
      " K Means Comparisons\n",
      "K means modularity on Gtopo: 0.09685293179840138\n",
      "\n",
      "Real district modularity on Gtopo\n",
      "116 partition on Gtopo: 0.0033596436850784075\n",
      "118 partition on Gtopo: 0.002097857237471587\n",
      "\n",
      " null model comparisons\n",
      "Random partition modularity on Gtopo: -0.00043794427740562625\n",
      "Equal partition modularity on Gtopo: -0.0004899222461252368\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n 10Graph Comparisons\")\n",
    "#print(\"Louvain modularity Gk:\",nx.algorithms.community.modularity(Gk, partGk, weight='weight'))\n",
    "print(\"Louvain modularity Gtopo:\",nx.algorithms.community.modularity(Gtopo, partGtopo, weight='weight'))\n",
    "#print(\"Louvain modularity GeoPure partition on Gk:\",nx.algorithms.community.modularity(Gk, pure_part, weight='weight'))\n",
    "print(\"Louvain modularity GeoPure partition on Gtopo:\",nx.algorithms.community.modularity(Gtopo, pure_part, weight='weight'))\n",
    "#print(\"Louvain modularity Gtopo on Gk:\",nx.algorithms.community.modularity(Gk, partGtopo, weight='weight'))\n",
    "#print(\"Louvain modularity Gk on Gtopo:\",nx.algorithms.community.modularity(Gtopo, partGk, weight='weight'))\n",
    "\n",
    "print(\"\\n K Means Comparisons\")\n",
    "#print(\"K means modularity on Gk:\",nx.algorithms.community.modularity(Gk, index_partitions, weight='weight'))\n",
    "print(\"K means modularity on Gtopo:\",nx.algorithms.community.modularity(Gtopo, index_partitions, weight='weight'))\n",
    "\n",
    "print(\"\\nReal district modularity on Gtopo\")\n",
    "#print(\"116 partition on Gk:\",nx.algorithms.community.modularity(Gk, part116, weight='weight'))\n",
    "#print(\"118 partition on Gk:\",nx.algorithms.community.modularity(Gk, part118, weight='weight'))\n",
    "print(\"116 partition on Gtopo:\",nx.algorithms.community.modularity(Gtopo, part116, weight='weight'))\n",
    "print(\"118 partition on Gtopo:\",nx.algorithms.community.modularity(Gtopo, part118, weight='weight'))\n",
    "\n",
    "print(\"\\n null model comparisons\")\n",
    "print(\"Random partition modularity on Gtopo:\",nx.algorithms.community.modularity(Gtopo, part_random, weight='weight'))\n",
    "print(\"Equal partition modularity on Gtopo:\",nx.algorithms.community.modularity(Gtopo, part_equal, weight='weight'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7dce39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 10Graph Comparisons\n",
      "K means loss of Gtopo: 119.70203635650712\n",
      "K means loss of GeoPure: 114.37685184972688\n",
      "\n",
      " K Means\n",
      "loss of k means: 29.0772838818855\n",
      "\n",
      "Real district k means loss\n",
      "116 partition: 115.18201653038571\n",
      "118 partition: 115.6993863980432\n",
      "\n",
      " null model k means loss\n",
      "Random partition k means loss: 119.90356916854324\n",
      "Equal partition k means loss: 119.94362779605896\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n 10Graph Comparisons\")\n",
    "#print(\"K means loss of Gk:\",calculate_weighted_kmeans_loss_for_partition(new_data, partGk, weights=var_explained))\n",
    "print(\"K means loss of Gtopo:\",calculate_weighted_kmeans_loss_for_partition(new_data, partGtopo, weights=var_explained))\n",
    "print(\"K means loss of GeoPure:\",calculate_weighted_kmeans_loss_for_partition(new_data, pure_part, weights=var_explained))\n",
    "\n",
    "print(\"\\n K Means\")\n",
    "print(\"loss of k means:\",calculate_weighted_kmeans_loss_for_partition(new_data, index_partitions, weights=var_explained))\n",
    "\n",
    "print(\"\\nReal district k means loss\")\n",
    "print(\"116 partition:\",calculate_weighted_kmeans_loss_for_partition(new_data, part116, weights=var_explained))\n",
    "print(\"118 partition:\",calculate_weighted_kmeans_loss_for_partition(new_data, part118, weights=var_explained))\n",
    "\n",
    "\n",
    "print(\"\\n null model k means loss\")\n",
    "print(\"Random partition k means loss:\",calculate_weighted_kmeans_loss_for_partition(new_data, part_random, weights=var_explained))\n",
    "print(\"Equal partition k means loss:\",calculate_weighted_kmeans_loss_for_partition(new_data, part_equal, weights=var_explained))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6aa2d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_group_weighted_modularity(G, group, total_graph_weight):\n",
    "\n",
    "    subgraph = G.subgraph(group)\n",
    "    L_c = subgraph.size(weight='weight')\n",
    "    S_c = sum(dict(G.degree(group, weight='weight')).values())\n",
    "\n",
    "    term1 = L_c / total_graph_weight\n",
    "\n",
    "    term2 = (S_c / (2 * total_graph_weight)) ** 2\n",
    "    \n",
    "    return term1 - term2\n",
    "\n",
    "# normalization_factor=sum([G[u][v][\"weight\"] for u,v in G.edges()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d59be3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod=0\n",
    "\n",
    "# for part in partG: # where\n",
    "#     mod+=one_group_weighted_modularity(G,part,normalization_factor)\n",
    "# print(\"Modularity calculated by one_group_modularity function:\", mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0dde6833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.algorithms.community.modularity(G, partG, weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8067c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# import numpy as np\n",
    "# import random\n",
    "\n",
    "# def get_modularity(G):\n",
    "#     \"\"\"Safely runs Louvain and returns modularity.\"\"\"\n",
    "#     if G.number_of_edges() == 0: return 0\n",
    "#     try:\n",
    "#         # Use resolution=1 as per your request\n",
    "#         part = nx.algorithms.community.louvain_communities(G, weight='weight', resolution=1)\n",
    "#         return nx.algorithms.community.modularity(G, part, weight='weight')\n",
    "#     except:\n",
    "#         return 0\n",
    "\n",
    "# def randomize_graph(G):\n",
    "#     \"\"\"\n",
    "#     Creates a random graph with the same Number of Nodes and Edges as G,\n",
    "#     and the exact same list of edge weights, but randomized topology.\n",
    "#     \"\"\"\n",
    "#     N = G.number_of_nodes()\n",
    "#     M = G.number_of_edges()\n",
    "    \n",
    "#     # 1. Extract exact weights from original graph\n",
    "#     weights = [d['weight'] for u, v, d in G.edges(data=True)]\n",
    "    \n",
    "#     # 2. Create random topology (Erdos-Renyi) with same density\n",
    "#     G_rand = nx.gnm_random_graph(N, M, seed=42)\n",
    "    \n",
    "#     # 3. Assign the original shuffled weights to the new random edges\n",
    "#     random.shuffle(weights)\n",
    "#     for i, (u, v) in enumerate(G_rand.edges()):\n",
    "#         G_rand[u][v]['weight'] = weights[i]\n",
    "        \n",
    "#     return G_rand\n",
    "\n",
    "# # --- LOOP 1: Varying K (Standard KNN) ---\n",
    "# print(\"\\n--- EXPERIMENT 1: Standard KNN (Gk) ---\")\n",
    "# print(f\"{'K':<4} | {'Edges':<10} | {'Q (Real)':<8} | {'Q (Rand)':<8} | {'Diff':<8}\")\n",
    "# print(\"-\" * 50)\n",
    "\n",
    "# for i in range(2, 20):\n",
    "#     # 1. Build Real Graph\n",
    "#     Gk = create_dimension_layered_knn(new_data, dimension_weights=var_explained, k=i)\n",
    "    \n",
    "#     # 2. Build Random Null Model\n",
    "#     Gk_random = randomize_graph(Gk)\n",
    "    \n",
    "#     # 3. Calculate Modularities\n",
    "#     q_real = get_modularity(Gk)\n",
    "#     q_rand = get_modularity(Gk_random)\n",
    "    \n",
    "#     # Calculate sum of weights for reference (Network Mass)\n",
    "#     total_weight = sum([d['weight'] for u, v, d in Gk.edges(data=True)])\n",
    "    \n",
    "#     print(f\"{i:<4} | {int(total_weight):<10} | {q_real:.4f}   | {q_rand:.4f}   | {q_real - q_rand:.4f}\")\n",
    "\n",
    "\n",
    "# # --- LOOP 2: Varying Radius (Topology Graph) ---\n",
    "# print(\"\\n--- EXPERIMENT 2: Smoothed Topology (Gtopo) ---\")\n",
    "# print(f\"{'r_max':<10} | {'Edges':<10} | {'Q (Real)':<8} | {'Q (Rand)':<8} | {'Diff':<8}\")\n",
    "# print(\"-\" * 55)\n",
    "\n",
    "# for i in range(1, 12):\n",
    "#     # 1. Parameters\n",
    "#     rmax = 0.00015 * 2**(i-3)\n",
    "#     rdisc = rmax / 4\n",
    "    \n",
    "#     # 2. Build Real Graph\n",
    "#     Gmulti = build_topology_graph(new_data, var_explained, rmax, rdisc, k=10, scaling_method=\"std\")\n",
    "#     Gtopo = flatten_graph_for_community_detection(Gmulti)\n",
    "    \n",
    "#     # 3. Build Random Null Model\n",
    "#     Gtopo_random = randomize_graph(Gtopo)\n",
    "    \n",
    "#     # 4. Calculate Modularities\n",
    "#     q_real = get_modularity(Gtopo)\n",
    "#     q_rand = get_modularity(Gtopo_random)\n",
    "    \n",
    "#     # Calculate sum of weights for reference\n",
    "#     total_weight = sum([d['weight'] for u, v, d in Gtopo.edges(data=True)])\n",
    "    \n",
    "#     print(f\"{rmax:<10.5f} | {int(total_weight):<10} | {q_real:.4f}   | {q_rand:.4f}   | {q_real - q_rand:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b65bf10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1, 12):\n",
    "#     # 1. Parameters\n",
    "#     rmax = 0.00015 * 2**(i-3)\n",
    "#     rdisc = rmax / 4\n",
    "    \n",
    "#     # 2. Build Real Graph\n",
    "#     Gmulti = build_topology_graph(new_data, var_explained, rmax, rdisc, k=10, scaling_method=\"std\")\n",
    "#     Gtopo = flatten_graph_for_community_detection(Gmulti)\n",
    "    \n",
    "#     # 3. Build Random Null Model\n",
    "#     Gtopo_random = randomize_graph(Gtopo)\n",
    "    \n",
    "#     # 4. Calculate Modularities\n",
    "#     q_real = get_modularity(Gtopo)\n",
    "#     q_rand = get_modularity(Gtopo_random)\n",
    "    \n",
    "#     # Calculate sum of weights for reference\n",
    "#     total_weight = sum([d['weight'] for u, v, d in Gtopo.edges(data=True)])\n",
    "    \n",
    "#     print(f\"{rmax:<10.5f} | {int(total_weight):<10} | {q_real:.4f}   | {q_rand:.4f}   | {q_real - q_rand:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429d5fa5",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "71e5c2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set, Dict, Tuple\n",
    "import time\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def mean_variance(group,node,new_data,weights):\n",
    "    n=len(group)\n",
    "    var=0\n",
    "    for i in range(new_data.shape[1]):\n",
    "        weight=weights[i]\n",
    "        dif=[new_data[node,i]-new_data[member,i] for member in group]\n",
    "        var+=weight*sum(x**2 for x in dif)/n\n",
    "    return var\n",
    "\n",
    "def partition_to_map(partition: List[Set[int]]) -> Dict[int, int]:\n",
    "    \"\"\"Converts a list of sets to a dictionary mapping node -> community_index.\"\"\"\n",
    "    node_to_community = {}\n",
    "    for i, community_set in enumerate(partition):\n",
    "        for node in community_set:\n",
    "            node_to_community[node] = i\n",
    "    return node_to_community\n",
    "\n",
    "def is_contiguous(G: nx.Graph, partition: List) -> bool:\n",
    "    for part in partition:\n",
    "        if not nx.is_connected(G.subgraph(part)):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# def is_contiguous(G: nx.Graph, groupA: Set, groupB: Set) -> bool:\n",
    "#     \"\"\"\n",
    "#     Checks if the two prospective communities (groupA and groupB) are contiguous\n",
    "#     in the geographical graph G.\n",
    "#     \"\"\"\n",
    "#     # Check contiguity for groupA\n",
    "#     # Only check if the group has more than one node\n",
    "#     if len(groupA) > 1 and not nx.is_connected(G.subgraph(groupA)):\n",
    "#         return False\n",
    "#     # Check contiguity for groupB\n",
    "#     if len(groupB) > 1 and not nx.is_connected(G.subgraph(groupB)):\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "def one_group_weighted_modularity(G, group, total_graph_weight):\n",
    "\n",
    "    subgraph = G.subgraph(group)\n",
    "    L_c = subgraph.size(weight='weight')\n",
    "    S_c = sum(dict(G.degree(group, weight='weight')).values())\n",
    "\n",
    "    term1 = L_c / total_graph_weight\n",
    "\n",
    "    term2 = (S_c / (2 * total_graph_weight)) ** 2\n",
    "    \n",
    "    return term1 - term2\n",
    "# normalization_factor=sum([G[u][v][\"weight\"] for u,v in G.edges()])\n",
    "\n",
    "def balance_population(Geo: nx.Graph, unbalanced_part: List[Set[int]], tol: int):\n",
    "    # Convert part to map for iterations\n",
    "    best_part = [set(comm) for comm in unbalanced_part]\n",
    "    node_to_community_map = partition_to_map(best_part)\n",
    "    # Get indicies where each district starts\n",
    "    district_indices = list(range(len(best_part)))\n",
    "    swap_flag = False\n",
    "    \n",
    "    # Iterate through all districts\n",
    "    for comm_A_idx in district_indices:\n",
    "        district_A_nodes = best_part[comm_A_idx]\n",
    "\n",
    "        # Iterate through each node in district_A\n",
    "        for node_A in district_A_nodes:\n",
    "            if node_A not in best_part[comm_A_idx]:\n",
    "                continue\n",
    "\n",
    "            # Iterate over all geographical neighbors of A\n",
    "            for node_B in Geo.neighbors(node_A):\n",
    "                comm_B_idx = node_to_community_map.get(node_B)\n",
    "\n",
    "                district_B_nodes = best_part[comm_B_idx]\n",
    "\n",
    "                if comm_B_idx is None or comm_A_idx == comm_B_idx:\n",
    "                    continue\n",
    "                \n",
    "                prospective_part = [set(comm) for comm in best_part]\n",
    "                # Check population equality\n",
    "                if np.abs(len(district_B_nodes) - len(district_A_nodes)) < tol:\n",
    "                    # Find which one is larger, simulate swap\n",
    "                    if len(district_A_nodes) < len(district_B_nodes):\n",
    "                        # B leaves comm_B, B added to comm_A\n",
    "                        prospective_part[comm_B_idx].discard(node_B)\n",
    "                        prospective_part[comm_A_idx].add(node_B)\n",
    "                    else:\n",
    "                        # A leaves comm_A\n",
    "                        prospective_part[comm_A_idx].discard(node_A)\n",
    "                        prospective_part[comm_B_idx].add(node_A)\n",
    "\n",
    "                    # Check contiguity of two groups\n",
    "                    if is_contiguous(Geo,prospective_part):\n",
    "                        # Update best part\n",
    "                        best_part = prospective_part\n",
    "                        # node_to_community_map = partition_to_map(best_part)\n",
    "                        # swap_count += 1\n",
    "                        # swap_flag = True\n",
    "                        # print(f\"Swap minimizes var, maintains contiguity. Node {node_A} and Node {node_B}.\")\n",
    "                        # print(f\"Swap minimizes var, maintains contiguity. Swap Count: {swap_count}.\")\n",
    "    return best_part\n",
    "\n",
    "def sequential_swap_var(G: nx.Graph, Geo: nx.Graph, current_partition: List[Set[int]]) -> Tuple[List[Set[int]], float]:\n",
    "    \n",
    "    iteration_time = time.time()\n",
    "    swap_count = 0\n",
    "    swap_flag = False\n",
    "\n",
    "    best_part = [set(comm) for comm in current_partition]\n",
    "    node_to_community_map = partition_to_map(best_part)\n",
    "\n",
    "\n",
    "    # 2. Iterate through each district (community)\n",
    "    # We use a list of indices to ensure we iterate over all districts\n",
    "    district_indices = list(range(len(best_part)))\n",
    "\n",
    "    # 3. Iterate through each district\n",
    "    for comm_A_idx in district_indices:\n",
    "        # best_swap = None  # Stores: (node_A, node_B, comm_A_idx, comm_B_idx)\n",
    "        \n",
    "        district_A_nodes = best_part[comm_A_idx]\n",
    "        \n",
    "        # Identify all potential swap candidates involving a node from comm_A_idx\n",
    "        # A swap candidate is a pair (A, B) where A is in comm_A_idx and B is adjacent to A\n",
    "        # and B is in a different community comm_B_idx.\n",
    "        \n",
    "        # Iterate over all nodes A in the current district\n",
    "        for node_A in district_A_nodes:\n",
    "\n",
    "            if node_A not in best_part[comm_A_idx]:\n",
    "                continue\n",
    "\n",
    "            # Iterate over all geographical neighbors of A\n",
    "            for node_B in Geo.neighbors(node_A):\n",
    "                comm_B_idx = node_to_community_map.get(node_B)\n",
    "\n",
    "                district_B_nodes = best_part[comm_B_idx]\n",
    "\n",
    "                if comm_B_idx is None or comm_A_idx == comm_B_idx:\n",
    "                    continue\n",
    "\n",
    "                var_A_now = mean_variance(district_A_nodes,node_A,new_data,weights)\n",
    "                var_B_now = mean_variance(district_B_nodes,node_B,new_data,weights)\n",
    "\n",
    "                var_A_swap = mean_variance(district_A_nodes,node_B,new_data,weights)\n",
    "                var_B_swap = mean_variance(district_B_nodes,node_A,new_data,weights)\n",
    "                \n",
    "                if var_A_swap + var_B_swap < var_A_now + var_B_now:\n",
    "                    # print(f\"Updating best swap. Swap variance {var_A_swap + var_B_swap} < Current variance {var_A_now + var_B_now}\")\n",
    "                    # Post-swap partition\n",
    "                    # Simulate the swap for the contiguity check\n",
    "                    prospective_part = [set(comm) for comm in best_part]\n",
    "\n",
    "                    # Swap\n",
    "\n",
    "                    # A leaves comm_A, B leaves comm_B\n",
    "                    prospective_part[comm_A_idx].discard(node_A)\n",
    "                    prospective_part[comm_B_idx].discard(node_B)\n",
    "\n",
    "                    # A enters comm_B, B enters comm_A\n",
    "                    prospective_part[comm_B_idx].add(node_A)\n",
    "                    prospective_part[comm_A_idx].add(node_B)\n",
    "\n",
    "                    # Check contiguity of two groups\n",
    "                    \n",
    "                    if is_contiguous(Geo,prospective_part[comm_A_idx], prospective_part[comm_B_idx]):\n",
    "                        # Update best part\n",
    "                        best_part = prospective_part\n",
    "                        node_to_community_map = partition_to_map(best_part)\n",
    "                        swap_count += 1\n",
    "                        swap_flag = True\n",
    "                        # print(f\"Swap minimizes var, maintains contiguity. Node {node_A} and Node {node_B}.\")\n",
    "                        # print(f\"Swap minimizes var, maintains contiguity. Swap Count: {swap_count}.\")\n",
    "\n",
    "    \n",
    "    iteration_time = time.time() - iteration_time\n",
    "\n",
    "    return best_part, iteration_time, swap_flag\n",
    "\n",
    "def sequential_swap_mod(G: nx.Graph, Geo: nx.Graph, current_partition: List[Set[int]], current_mod: float) -> Tuple[List[Set[int]], float]:\n",
    "    # swapping with modularity\n",
    "    iteration_time = time.time()\n",
    "    swap_count = 0\n",
    "    swap_flag = False\n",
    "\n",
    "    best_part = [set(comm) for comm in current_partition]\n",
    "    best_mod = current_mod\n",
    "    node_to_community_map = partition_to_map(best_part)\n",
    "\n",
    "\n",
    "    # 2. Iterate through each district (community)\n",
    "    # We use a list of indices to ensure we iterate over all districts\n",
    "    district_indices = list(range(len(best_part)))\n",
    "\n",
    "    # 3. Iterate through each district\n",
    "    for comm_A_idx in district_indices:\n",
    "        # best_swap = None  # Stores: (node_A, node_B, comm_A_idx, comm_B_idx)\n",
    "        \n",
    "        district_A_nodes = best_part[comm_A_idx]\n",
    "        \n",
    "        # Identify all potential swap candidates involving a node from comm_A_idx\n",
    "        # A swap candidate is a pair (A, B) where A is in comm_A_idx and B is adjacent to A\n",
    "        # and B is in a different community comm_B_idx.\n",
    "        \n",
    "        # Iterate over all nodes A in the current district\n",
    "        for node_A in district_A_nodes:\n",
    "\n",
    "            if node_A not in best_part[comm_A_idx]:\n",
    "                continue\n",
    "\n",
    "            # Iterate over all geographical neighbors of A\n",
    "            for node_B in Geo.neighbors(node_A):\n",
    "                comm_B_idx = node_to_community_map.get(node_B)\n",
    "\n",
    "                district_B_nodes = best_part[comm_B_idx]\n",
    "\n",
    "                # Skip checking same nodes\n",
    "                if comm_B_idx is None or comm_A_idx == comm_B_idx:\n",
    "                    continue\n",
    "\n",
    "                # Simulate the swap for the contiguity and modularity check\n",
    "                prospective_part = [set(comm) for comm in best_part]\n",
    "\n",
    "                # A leaves comm_A, B leaves comm_B\n",
    "                prospective_part[comm_A_idx].discard(node_A)\n",
    "                prospective_part[comm_B_idx].discard(node_B)\n",
    "\n",
    "                # A enters comm_B, B enters comm_A\n",
    "                prospective_part[comm_B_idx].add(node_A)\n",
    "                prospective_part[comm_A_idx].add(node_B)\n",
    "\n",
    "                # Check contiguity of two groups\n",
    "                if is_contiguous(Geo,prospective_part):\n",
    "                    # Passed contiguity check, check if new_mod is better than current_mod\n",
    "                    total_graph_weight = G.size(weight='weight')\n",
    "                    new_mod = one_group_weighted_modularity(G, current_partition, total_graph_weight)\n",
    "                    if new_mod > best_mod:\n",
    "                        # Update best part\n",
    "                        best_part = prospective_part\n",
    "                        node_to_community_map = partition_to_map(best_part)\n",
    "                        best_mod = new_mod\n",
    "                        swap_count += 1\n",
    "                        swap_flag = True\n",
    "                        # print(f\"Swap minimizes var, maintains contiguity. Node {node_A} and Node {node_B}.\")\n",
    "                        # print(f\"Swap minimizes var, maintains contiguity. Swap Count: {swap_count}.\")\n",
    "\n",
    "    \n",
    "    iteration_time = time.time() - iteration_time\n",
    "\n",
    "    return best_part, best_mod, iteration_time, swap_flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4db8eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Optimization Function with Sequential Swap and Delta Q ---\n",
    "def optimized_sequential_swap(G: nx.Graph, Geo: nx.Graph, N_iterations: int, n_swaps: int) -> Tuple[List[Set[int]], float]:\n",
    "    \"\"\"\n",
    "    Performs local search optimization using a sequential, targeted two-node swap.\n",
    "    In each iteration, it finds the single best swap across all districts and executes it.\n",
    "    \"\"\"\n",
    "    \n",
    "    opt_best_score = 0.0\n",
    "    opt_best_part = 0.0\n",
    "\n",
    "    total_duration = time.time()\n",
    "    \n",
    "    # --- Main Optimization Loop ---\n",
    "    for i in range(N_iterations):\n",
    "        print(f\"--- Iteration {i+1}/{N_iterations} ---\")\n",
    "        # Balance partition for population\n",
    "        iter_graph, iter_unbalance_partition = generate_geopure()\n",
    "        iter_initial_partition = balance_population(Geo,iter_unbalance_partition, tol=20)\n",
    "        \n",
    "        # Convert to map\n",
    "        iter_best_part = [set(comm) for comm in iter_initial_partition] # current best partition\n",
    "\n",
    "        # Calculate initial modularity score\n",
    "        iter_init_score = nx.algorithms.community.modularity(G, iter_best_part, weight='weight')\n",
    "        iter_best_score = iter_init_score\n",
    "        \n",
    "        iter_time = 0.0\n",
    "        for i in range(n_swaps):\n",
    "            # Swap\n",
    "            iter_new_part, swap_time, iter_new_mod, swap_flag = sequential_swap_mod(G, Geo, iter_best_part, iter_best_score)\n",
    "            iter_time = total_duration + swap_time\n",
    "\n",
    "            # If no swap occurred, optimization finished\n",
    "            if not swap_flag:\n",
    "                print(\"\\nNo local swap increased modularity while maintaining contiguity. Optimization finished.\")\n",
    "                break\n",
    "\n",
    "            # If new partition has better modularity score than current best, update current best\n",
    "            if iter_new_mod > iter_best_score:\n",
    "                iter_best_part = iter_new_part\n",
    "\n",
    "        # If current initial partition yielded best results, update opt best\n",
    "        opt_best_part = iter_best_part\n",
    "        opt_best_score = iter_best_score\n",
    "        print(f\"Iteration {i+1} complete. Time: {iter_time:.2f} seconds.\")\n",
    "        total_duration = total_duration + iter_time\n",
    "\n",
    "    # Calculate final modularity score\n",
    "    # print(f\"Length: {len(best_part)}\")\n",
    "    actual_opt_best_score = nx.algorithms.community.modularity(G, opt_best_part, weight='weight')\n",
    "    if not actual_opt_best_score == opt_best_score:\n",
    "        print(f\"Tracked modularity score ({actual_opt_best_score:.4f}) and final calculated modularity score ({opt_best_score}) not equal.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Final Results ---\")\n",
    "    print(f\"Optimal Modularity Score (after local swaps): {opt_best_score:.4f}\")\n",
    "    print(f\"Optimization took: {total_duration:.2f} seconds\")\n",
    "    return opt_best_part, opt_best_score, total_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee777fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 1/1 ---\n"
     ]
    },
    {
     "ename": "NetworkXError",
     "evalue": "Node {3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 5240, 5241, 5242, 5243, 5244, 5245, 5246, 5247, 5248, 5249, 5250, 5251, 5252, 5253, 5254, 5255, 5256, 1078, 1076, 1107, 1108, 3388, 1109, 2550, 1068, 1069, 1070, 1071, 1072, 1073, 1110, 1074, 1075, 3366, 3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 1111, 1079, 3380, 3381, 3384, 3385, 1080, 3387, 3382, 3389, 3390, 1081, 3383, 3386, 3392, 1077, 1082, 3391, 1083, 1112, 1084, 1085, 1086, 1087, 1113, 1088, 1089, 1090, 1091, 1092, 2551, 1093, 1114, 1094, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 2547, 2548, 2549, 3716, 3719, 3720, 3721, 3731, 3732, 3733, 3734, 3735, 3736, 3737, 3738, 3739, 3740, 3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749, 3750, 3751, 3752, 3753, 3754, 3755, 3756, 3757, 3758, 3759, 3760, 3761, 3762, 3763, 3764, 3765, 3766, 3767, 3768, 3769, 3770, 3771, 3772, 3773, 3774, 3775, 2552, 2553, 2554, 2546, 5938, 5939, 5940, 5941, 5942, 5943, 5944, 5945, 5946, 5947, 5948, 5949, 5950, 5951, 5952, 5953, 5954, 5955, 5956, 5957, 5958, 5959, 5960, 5961, 5962, 5963, 5964, 5965, 5966, 5967, 5968, 5969, 5970, 5971, 5972, 5973, 5974, 5975, 5976, 5977, 5978, 5979, 5668, 5670, 5671, 5672, 7038, 7039, 7040, 7041, 7042, 7043, 7044, 5673, 5674, 5675, 5676, 5677, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071} in sequence nbunch is not a valid node.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/networkx/classes/graph.py:2013\u001b[0m, in \u001b[0;36mGraph.nbunch_iter.<locals>.bunch_iter\u001b[0;34m(nlist, adj)\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nlist:\n\u001b[0;32m-> 2013\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m:\n\u001b[1;32m   2014\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m n\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'set'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNetworkXError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [85]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_partition, final_score, duration \u001b[38;5;241m=\u001b[39m \u001b[43moptimized_sequential_swap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGtopo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGeoPure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_swaps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Partition Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(best_partition)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Modularity Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36moptimized_sequential_swap\u001b[0;34m(G, Geo, N_iterations, n_swaps)\u001b[0m\n\u001b[1;32m     27\u001b[0m iter_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_swaps):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Swap\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     iter_new_part, swap_time, iter_new_mod, swap_flag \u001b[38;5;241m=\u001b[39m \u001b[43msequential_swap_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGeo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miter_best_part\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miter_best_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     iter_time \u001b[38;5;241m=\u001b[39m total_duration \u001b[38;5;241m+\u001b[39m swap_time\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# If no swap occurred, optimization finished\u001b[39;00m\n",
      "Input \u001b[0;32mIn [82]\u001b[0m, in \u001b[0;36msequential_swap_mod\u001b[0;34m(G, Geo, current_partition, current_mod)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_contiguous(Geo,prospective_part):\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# Passed contiguity check, check if new_mod is better than current_mod\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     total_graph_weight \u001b[38;5;241m=\u001b[39m G\u001b[38;5;241m.\u001b[39msize(weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 239\u001b[0m     new_mod \u001b[38;5;241m=\u001b[39m \u001b[43mone_group_weighted_modularity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_partition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_graph_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_mod \u001b[38;5;241m>\u001b[39m best_mod:\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;66;03m# Update best part\u001b[39;00m\n\u001b[1;32m    242\u001b[0m         best_part \u001b[38;5;241m=\u001b[39m prospective_part\n",
      "Input \u001b[0;32mIn [82]\u001b[0m, in \u001b[0;36mone_group_weighted_modularity\u001b[0;34m(G, group, total_graph_weight)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_group_weighted_modularity\u001b[39m(G, group, total_graph_weight):\n\u001b[0;32m---> 45\u001b[0m     subgraph \u001b[38;5;241m=\u001b[39m \u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubgraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     L_c \u001b[38;5;241m=\u001b[39m subgraph\u001b[38;5;241m.\u001b[39msize(weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     47\u001b[0m     S_c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mdict\u001b[39m(G\u001b[38;5;241m.\u001b[39mdegree(group, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/networkx/classes/graph.py:1820\u001b[0m, in \u001b[0;36mGraph.subgraph\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m   1763\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubgraph\u001b[39m(\u001b[38;5;28mself\u001b[39m, nodes):\n\u001b[1;32m   1764\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a SubGraph view of the subgraph induced on `nodes`.\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \n\u001b[1;32m   1766\u001b[0m \u001b[38;5;124;03m    The induced subgraph of the graph contains the nodes in `nodes`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;124;03m    [(0, 1), (1, 2)]\u001b[39;00m\n\u001b[1;32m   1819\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1820\u001b[0m     induced_nodes \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnbunch_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1821\u001b[0m     \u001b[38;5;66;03m# if already a subgraph, don't make a chain\u001b[39;00m\n\u001b[1;32m   1822\u001b[0m     subgraph \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39msubgraph_view\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/networkx/classes/filters.py:52\u001b[0m, in \u001b[0;36mshow_nodes.__init__\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, nodes):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/networkx/classes/graph.py:2027\u001b[0m, in \u001b[0;36mGraph.nbunch_iter.<locals>.bunch_iter\u001b[0;34m(nlist, adj)\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhashable\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m message:\n\u001b[1;32m   2024\u001b[0m     exc \u001b[38;5;241m=\u001b[39m NetworkXError(\n\u001b[1;32m   2025\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in sequence nbunch is not a valid node.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2026\u001b[0m     )\n\u001b[0;32m-> 2027\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[0;31mNetworkXError\u001b[0m: Node {3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 5240, 5241, 5242, 5243, 5244, 5245, 5246, 5247, 5248, 5249, 5250, 5251, 5252, 5253, 5254, 5255, 5256, 1078, 1076, 1107, 1108, 3388, 1109, 2550, 1068, 1069, 1070, 1071, 1072, 1073, 1110, 1074, 1075, 3366, 3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 1111, 1079, 3380, 3381, 3384, 3385, 1080, 3387, 3382, 3389, 3390, 1081, 3383, 3386, 3392, 1077, 1082, 3391, 1083, 1112, 1084, 1085, 1086, 1087, 1113, 1088, 1089, 1090, 1091, 1092, 2551, 1093, 1114, 1094, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 2547, 2548, 2549, 3716, 3719, 3720, 3721, 3731, 3732, 3733, 3734, 3735, 3736, 3737, 3738, 3739, 3740, 3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749, 3750, 3751, 3752, 3753, 3754, 3755, 3756, 3757, 3758, 3759, 3760, 3761, 3762, 3763, 3764, 3765, 3766, 3767, 3768, 3769, 3770, 3771, 3772, 3773, 3774, 3775, 2552, 2553, 2554, 2546, 5938, 5939, 5940, 5941, 5942, 5943, 5944, 5945, 5946, 5947, 5948, 5949, 5950, 5951, 5952, 5953, 5954, 5955, 5956, 5957, 5958, 5959, 5960, 5961, 5962, 5963, 5964, 5965, 5966, 5967, 5968, 5969, 5970, 5971, 5972, 5973, 5974, 5975, 5976, 5977, 5978, 5979, 5668, 5670, 5671, 5672, 7038, 7039, 7040, 7041, 7042, 7043, 7044, 5673, 5674, 5675, 5676, 5677, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071} in sequence nbunch is not a valid node."
     ]
    }
   ],
   "source": [
    "best_partition, final_score, duration = optimized_sequential_swap(Gtopo, GeoPure, N_iterations=1, n_swaps=10)\n",
    "print(f\"Final Partition Size: {len(best_partition)}\")\n",
    "print(f\"Final Modularity Score: {final_score}\")\n",
    "print(f\"Optimization Time: {duration}\")\n",
    "\n",
    "# Problem before going to the next problem:\n",
    "# 1. Fix errors\n",
    "# 2. Check if balance population works\n",
    "\n",
    "\n",
    "# Problem: Stuck at local minima\n",
    "# Try:\n",
    "# 1. Multiple initial positions\n",
    "# 2. Introduce noise via SGD\n",
    "# 3. Taking uphill steps every so often, probability of uphill step decreases over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc04f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_contiguous(G: nx.Graph, partition: List) -> bool:\n",
    "    for part in partition:\n",
    "        if not nx.is_connected(G.subgraph(part)):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3488cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
