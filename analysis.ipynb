{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de67e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def reduce_pca_by_variance(data: np.ndarray, variance_threshold: float):\n",
    "    \"\"\"\n",
    "    Performs PCA on n-dimensional data, automatically selecting the minimum\n",
    "    number of components to explain at least the `variance_threshold`.\n",
    "\n",
    "    Args:\n",
    "        data: A (n_samples, n_features) NumPy array.\n",
    "        variance_threshold: The target amount of variance to explain\n",
    "                            (e.g., 0.95 for 95%).\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - data_transformed (np.ndarray): The data projected onto the\n",
    "                                         new component space.\n",
    "        - fitted_pca (PCA): The fitted PCA object, which you can use\n",
    "                            to inspect the number of components, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Create a PCA object with the variance threshold.\n",
    "    # By setting n_components to a float, PCA automatically finds\n",
    "    # the components needed to explain that much variance.\n",
    "    pca = PCA(n_components=variance_threshold)\n",
    "    \n",
    "    # 2. Create a pipeline to first scale the data, then run PCA.\n",
    "    # Scaling is crucial for PCA to work correctly.\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', pca)\n",
    "    ])\n",
    "    \n",
    "    # 3. Fit the pipeline to the data and transform it\n",
    "    data_transformed = pipeline.fit_transform(data)\n",
    "    \n",
    "    # Return the new data and the fitted PCA object (which is stored in\n",
    "    # the 'pca' variable) so you can inspect it later.\n",
    "    return data_transformed, pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8919de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data to be a numpy array probably\n",
    "data=np.array(pd.read_csv('data.csv'))\n",
    "# Transform the income dimension by logarithmic scale\n",
    "data[:, income_index] = np.log(data[:, income_index])\n",
    "# Normalize data by dividing by standard deviation by dimension\n",
    "for i in range(data.shape[1]):\n",
    "    data[:, i] = data[:, i] / np.std(data[:, i])\n",
    "\n",
    "# Perform PCA, and project onto the top N dimensions so that they explain 50% of the variance\n",
    "new_data,pca = reduce_pca_by_variance(data, variance_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c740d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(data, weights,window_size):\n",
    "    # First add each data point as a node\n",
    "    G = nx.Graph()\n",
    "    for i, point in enumerate(data):\n",
    "        G.add_node(i, features=point)\n",
    "\n",
    "    # For each dimension in the data, add edges between points within a certain window size\n",
    "    \n",
    "    for dim in range(data.shape[1]):\n",
    "        weight=weights[dim]\n",
    "        # Find the max of the current dimension\n",
    "        dim_values = data[:, dim]\n",
    "        max_value = np.max(dim_values)\n",
    "        \n",
    "        n=int(max_value/window_size) + 1\n",
    "        \n",
    "        # this add each marker in the dimesion as a node, so that data points close to it can connect to it\n",
    "        markers = [window_size * i for i in range(1,n+1)]\n",
    "        G.add_node(f'{dim,markers[0]:.4}', marker=True, dim=dim, value=markers[0])\n",
    "        for i in range(1,n+1):\n",
    "            G.add_node(f'{dim,markers[i]:.4}', marker=True, dim=dim, value=markers[i])\n",
    "            G.add_edge(f'{dim,markers[i-1]:.4}',f'{dim,markers[i]:.4}', weight=weight)\n",
    "        \n",
    "        # Now connect data points to the nearest marker nodes within the window size\n",
    "        for i, point in enumerate(data):\n",
    "            point_value = point[dim]\n",
    "            lower_marker = markers[int(point_value // window_size)]\n",
    "            G.add_edge(i, f'{dim,lower_marker:.4}', weight=weight)\n",
    "        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812afc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_kmeans_partition(data: np.ndarray, weights, n_clusters=14):\n",
    "    \"\"\"\n",
    "    Runs K-means clustering on the input data and returns the loss\n",
    "    (inertia) and a partition of the data indices by cluster.\n",
    "\n",
    "    Args:\n",
    "        data: A (n_samples, n_features) NumPy array.\n",
    "        n_clusters: The number of clusters (k).\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - loss (float): The inertia (Within-Cluster Sum of Squares).\n",
    "        - partitions (dict): A dictionary where keys are cluster IDs (0 to k-1)\n",
    "                             and values are lists of original data indices\n",
    "                             belonging to that cluster.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Initialize and fit the K-means model\n",
    "    # n_init=10 runs the algorithm 10 times and picks the best result\n",
    "    # random_state=42 ensures the result is reproducible\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    for i in range(weights):\n",
    "        data[:, i] = data[:, i] * weights[i]\n",
    "    kmeans.fit(data)\n",
    "\n",
    "    # 2. Get the loss (inertia)\n",
    "    # .inertia_ is the WCSS (Within-Cluster Sum of Squares)\n",
    "    loss = kmeans.inertia_\n",
    "\n",
    "    # 3. Get the cluster assignment for each data point\n",
    "    # .labels_ is an array like [0, 1, 1, 0, 2, ...]\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # 4. Create the partition of indices\n",
    "    partitions = {i: [] for i in range(n_clusters)}\n",
    "    for index, cluster_id in enumerate(labels):\n",
    "        partitions[cluster_id].append(index)\n",
    "\n",
    "    return loss, partitions\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# 1. Create a sample NumPy matrix (6 samples, 2 features)\n",
    "# Let's create two clear clusters\n",
    "\n",
    "k = 14\n",
    "\n",
    "# 3. Run the function\n",
    "total_loss, index_partitions = get_kmeans_partition(new_data,weights, n_clusters=k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba7ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "G=create_network(new_data, weights=[], window_size=0.1)\n",
    "# Give it a partition from the NC districts\n",
    "# We can give it several partitions from the recent redistrictings and test the modularity for each.s\n",
    "partition=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3468305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.algorithms.community.louvain_communities(G,weight='weight')\n",
    "nx.algorithms.community.modularity(G, partition, weight='weight')\n",
    "nx.algorithms.community.modularity(G, index_partitions, weight='weight')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
